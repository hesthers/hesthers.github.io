{"meta":{"title":"My Blog for coding and note","subtitle":"https://github.com/hesthers","description":"python, git/github, data analysis, chinese","author":"hs_h_esther","url":"https://hesthers.github.io/archives","root":"https://hesthers.github.io/"},"pages":[],"posts":[{"title":"Algorithm(pt.2)","slug":"Algorithm-pt-2","date":"2022-10-25T06:00:28.000Z","updated":"2022-10-25T06:10:02.680Z","comments":true,"path":"2022/10/25/Algorithm-pt-2/","link":"","permalink":"https://hesthers.github.io/archives/2022/10/25/Algorithm-pt-2/","excerpt":"","text":"최단 경로 알고리즘 가장 짧은 경로를 찾는 것 한 지점 → 한 지점 한 지점 → 모든 지점 모든 지점 → 모든 지점 지점 = 노드 (예. 마을, 국가 등..) 이동 가능함 = 간선으로 표시 다익스트라 최단 경로 알고리즘 특정 한 노드 → 다른 모든 노드 계산 그리디 알고리즘으로 가장 비용이 적은 노드 선택 동작 과정 출발 노드 설정 최단 거리 테이블 초기화 (자신의 노드 = 0, 그 외 노드 거리 = $\\infty$) 방문하지 않은 노드에서 최단 거리가 가장 짧은 노드 선택하고 다른 노드로 가는 비용 계산하는 과정 반복 동일 비용을 갖는 경우 더 작은 수를 갖는 노드부터 우선 계산 마지막 노드 정보를 처리하지 않아도 상관없음 (이전의 정보를 활용하여 이미 가장 짧은 노드의 거리를 갱신했기 때문) 처리 과정 중 더 짧은 노드가 존재하면 해당 노드를 최단 거리 노드로 갱신 한 번 처리된 노드의 최단 거리는 고정 → 변화 없음 노드 탐색 시 BFS 이용하는 알고리즘 우선순위 큐 우선순위가 높은 데이터를 먼저 삭제 힙 (최소 힙, 최대 힙) 다익스트라 알고리즘에서 사용됨. (시간을 개선시키기 위함) 파이썬: heapq 라이브러리 사용 가능 (min heap 사용) ⇒ 다익스트라 알고리즘에서 사용됨. — 참고: 파이썬 알고리즘 인터뷰 책 &amp; 이코테 2021 강의","categories":[],"tags":[]},{"title":"TIL_algorithm","slug":"TIL-algorithm","date":"2022-09-06T13:28:09.000Z","updated":"2022-09-09T14:43:02.547Z","comments":true,"path":"2022/09/06/TIL-algorithm/","link":"","permalink":"https://hesthers.github.io/archives/2022/09/06/TIL-algorithm/","excerpt":"","text":"TIL Algorithm study DFS &amp; BFS 스택 &amp; 큐 Stack 선입후출 형식삽입-삭제 방식 사용 Queue 선입선출 형식삽입-삭제 방식 사용예. 은행의 창구 대기열python from collections import deque 라이브러리 사용 재귀함수 (Recursion) 최대 재귀 깊이 설정 미리 필요함(종료조건) =&gt; 무한 호출 가능성스택 방식 사용하여 스택 대신 사용예. 팩토리얼 계산예. 유클리드 호제법- 최대공약수 계산 R = A % B; GCD(A, B) = GCD(B, R)복잡한 알고리즘 혹은 반복문 사용을 간결하게 하여 시간의 효율성 보장 DFS &amp; BFS DFS(Depth-First Search) 깊이 우선 탐색스택/재귀함수 구조 사용시작노드부터 스택 삽입 후 방문 처리 BFS(Breadth-First Search) 너비 우선 탐색큐 자료 구조 사용방무 노드를 미리 방문 처리 후 방문하지 않은 노드를 삽입, 방문 처리 TIL 알고리즘 문제 풀이 (백준/프로그래머스)—&gt; 코테 대비 프로그래머스 문제 1~2개씩 풀기 (매일!!)","categories":[],"tags":[{"name":"TIL","slug":"TIL","permalink":"https://hesthers.github.io/archives/tags/TIL/"}]},{"title":"TIL (today study)","slug":"TIL-today-study","date":"2022-07-06T14:44:04.000Z","updated":"2022-07-06T14:47:22.222Z","comments":true,"path":"2022/07/06/TIL-today-study/","link":"","permalink":"https://hesthers.github.io/archives/2022/07/06/TIL-today-study/","excerpt":"","text":"TIL오랜만에 돌아온 TIL 시리즈공부 뭘 했는지 기록하기 위한 나의 개인 기록 포스트 LG Aimers part.1에 해당하는 ch.1~2 강의 다시 듣고 복습하기 패캠 코테 준비 강의 (탐색 알고리즘까지 마무리) 백준 연습문제 풀이 랭디 중국어 수업","categories":[],"tags":[]},{"title":"new language study","slug":"new-language-study","date":"2022-04-01T13:47:20.000Z","updated":"2022-04-01T14:33:30.750Z","comments":true,"path":"2022/04/01/new-language-study/","link":"","permalink":"https://hesthers.github.io/archives/2022/04/01/new-language-study/","excerpt":"","text":"Start studying new computer language, JAVA.— watched online lecture (FAST CAMPUS): 한 번에 끝내는 코딩테스트 369 초격차 패키지 Online How to install JAVA on Jupyter notebook JAVA data structures: Array Array data type use index number declare using []: empty array fixed length of one array print() in python = System.out.printin(); print total array: Array.toString() 1,2,3-D arrays available","categories":[],"tags":[]},{"title":"my thinking note","slug":"my-thinking-note","date":"2022-03-16T13:47:01.000Z","updated":"2022-03-16T14:09:40.124Z","comments":true,"path":"2022/03/16/my-thinking-note/","link":"","permalink":"https://hesthers.github.io/archives/2022/03/16/my-thinking-note/","excerpt":"","text":"Thinking note (생각노트)이 블로그 포스트로 내 의식의 흐름(??)대로 생각을 적어보려고 한다. 거의 두 달만에 올리게 되는 것 같다. 그 동안 많은 일들이 있었고 나의 심경 또한 변화하기도 했다..슬픔, 기회는 어느 한 순간에 찾아오는 것 같다. 내가 살고 있는 지금 이 순간들을 한번씩 돌아보면서 생각해보게 되는 것 같다. 내가 잘할 수 있는 일이 무엇인지, 좋아하는 게 무엇인지 계속해서 고민해보고 있고 내가 순간순간을 잘 보내고 있는지도 문득 돌아보게 된다. 슬픔 그리고 행복 이 두 가지들이 나의 감정을 왔다갔다하게 만든다. 아직까지 안정되지 못한 현재 나의 모습에 때로는 불안감을 느끼기도 한다. 이 불안감이 나를 더욱 불안하게 만든다. 이 이틀 동안 꾼 꿈들은 해몽은 너무 좋은데 해몽한 대로 좋은 소식이 나를 기다리고 있을 지 의문이 든다. 불안감 때문에 더 그런 것 같다. 계속해서 이메일, 홈페이지, 그리고 휴대폰을 매일 매 시간마다 들여다보며 혹시 연락이 없는 지 계속해서 보게 된다.. 내가 기회를 놓치지 않으려고 잡았는데 그 기회가 나를 놓는 것은 아닌지.. 괜찮을 거야라고 나 자신을 다독여보기는 하지만 마음 한 편에서는 왜 자꾸만 불편한 느낌이 드는 걸까. 내 노력들이 부족한 건지도 어쩌면 내가 너무 안일하게 생각했던 것은 아닐까. 나 나름대로 잘 해오고 있다라고 느끼면서도 다시 돌아보면 또 그렇지도 않은 것 같아 보인다. 나는 과연 나 자신을 신뢰하고 있는 걸까? 내 삶은 지금 얼마나 가치가 있는가? “ 사람이 온다는 것은사실은 어마어마한 일이다.그는그의 과거와현재와그리고그의 미래와 함께 오기 때문이다.한 사람의 일생이 오기 때문이다.”[[방문객 by. 정현종]]","categories":[],"tags":[]},{"title":"FM and autoencoder model","slug":"FM-and-autoencoder-model","date":"2022-01-20T13:28:20.000Z","updated":"2022-01-20T13:37:47.735Z","comments":true,"path":"2022/01/20/FM-and-autoencoder-model/","link":"","permalink":"https://hesthers.github.io/archives/2022/01/20/FM-and-autoencoder-model/","excerpt":"","text":"프로젝트가 거의 마무리 되어가고 있다. 마무리 되어가고 있는 현 시점에서 한 번 짚고 넘어갈만한 부분을 정리해볼까 싶다. SMOTE로 불균형 처리된 데이터로 학습을 시키면 확실히 성능이 올라가는 것을 볼 수 있다. 현재 이용하고 있는 데이터 자체가 불균형이 상당히 심해서 불균형 처리를 하지 않으면 성능이 상당히 낮게 나온다. 검증데이터에 한해서 불균형 처리를 하면 성능은 괜찮은데 테스트 데이터는 음… FM model 성능이 autoencoder model 보다 좀 괜찮게 나온다. (논문에서는 오토인코더가 괜찮다고 했던 것 같은데 데이터 문제로 성능이 차이가 있는 것처럼 보여진다.)","categories":[],"tags":[{"name":"project","slug":"project","permalink":"https://hesthers.github.io/archives/tags/project/"}]},{"title":"feedback regarding autoencoder","slug":"feedback-regarding-autoencoder","date":"2022-01-14T05:28:58.000Z","updated":"2022-01-14T09:26:45.635Z","comments":true,"path":"2022/01/14/feedback-regarding-autoencoder/","link":"","permalink":"https://hesthers.github.io/archives/2022/01/14/feedback-regarding-autoencoder/","excerpt":"","text":"Today’s feedback: 오토인코더 성능을 좋게 할 수 있는 방법은? 오토인코더 중에서 좋은 성능을 가진 모델은 무엇인가? 하이퍼 파라미터 튜닝 = latent feature 수, epoch 수를 늘리는 것. 코드가 잘못 ⇒ accuracy 0 (다 맞는데 라벨을 잘못.) 0.5보다 낮으면 코드가 틀렸을 경우 혹은 major class를 못 맞추는 경우 classification report에서 잘못된 게 있는지, overfitting의 가능성이 높음 드롭아웃으로 학습이 안되었을 때에도 존재하지만 성능이 완전 나쁠 수는 없음. 심한 드롭아웃을 하더라도 output은 랜덤샘플링에 가까운 것으로 나와야 함. (0.5에 근사하게.) 성능이 0 = 결과 계산부분에서 이슈가 있을 수 있으므로 확인 필요함. data 분포에 상황에 따라 다름. convolutional &amp; denoising autoencoder가 부수적으로.. 불균형 데이터를 처리하고 나서 딥러닝 모델의 성능이 나빠질 수 있는 경우 존재?? 가능함. 확인 사항: 오토인코더의 성능(latent feature이 좋은지 - 오토인코더 l2 loss 먼저 확인)과 upsampling의 영향이 충분했는가 (분류성능에서는 좋지 않음. upsampling 먼저 → test 성능 확인 먼저, stratify 비율을 가지고 확인, 학습에서는 upsampling ⇒ 최종에서는 원본 데이터 셋으로) 오토 인코더에서 dense unit값이 늘어나면 성능이 나빠질 수도 있는지 여부 오토인코더의 학습 먼저 확인 → l2 loss가 낮아지도록 하는 방향 → decode → 학습을 다시 (따로 학습) 일반적으로 latent layer를 늘리거나 하지 않음. 성능 문제 (낮은 accuracy 문제) 해결!!","categories":[],"tags":[]},{"title":"self-affirmation","slug":"self-affirmation","date":"2022-01-13T13:02:59.000Z","updated":"2022-01-13T13:09:37.983Z","comments":true,"path":"2022/01/13/self-affirmation/","link":"","permalink":"https://hesthers.github.io/archives/2022/01/13/self-affirmation/","excerpt":"","text":"작년부터 계획했던 것들이 약간씩 뒤틀리게 되면서 뭔가 혼란스럽기도 하고 계획했던 것들을 꼭 이뤄야만 한다고 생각해서인지 이렇게 포기해도 되는 건가 싶다. 뭔지 모르는 두려움? 같은 감정이 밀려온다.. 올해는 좋은 일만 가득하길.. 좋은 일들만 가득하길 바란다. 지금 하고 있는 모든 것들이 나에게는 좋은 결과를 가져올 것이라 생각한다. 결과가 의미있는 것이 훨씬 더 중요하니까. 올해는 내 소망했던 많은 것들을 이룰 수 있기를!!","categories":[],"tags":[]},{"title":"feedback of a project","slug":"feedback-of-a-project","date":"2022-01-08T13:21:26.000Z","updated":"2022-01-08T13:24:47.832Z","comments":true,"path":"2022/01/08/feedback-of-a-project/","link":"","permalink":"https://hesthers.github.io/archives/2022/01/08/feedback-of-a-project/","excerpt":"","text":"기존의 코드를 확인해서 그대로 사용가능하고 문제가 있는 경우 코드 수정이 필요. stack of flow 빠르게 오픈소스 코드를 가지고 와서 구현할 수 있는지 새로 나오는 딥러닝에 관한 논문들을 보고 바로바로 이해할 수 있고 코드로 구현할 수 있는가 (수준이 높음) 딥러닝/머신러닝의 차이 = feature extraction (딥러닝이 알아서 줄여줌 = embedding) fully connected layer: 고차원 → 저차원으로… input vector ⇒ linear classifier로 변경 학습이 잘 되는 vector = 0,1로 잘 구분된 것. model capacity가 크다 = 딥러닝 딥러닝은 feature vector가 많음 (고차원) class imbalanced ⇒ 모델 자체로는 학습하기 어려운 상태이므로 minority class를 어느 정도 보완해줘야 함. (일반적으로 배너광고를 잘 클릭하지 않음. = 극심한 class imbalanced의 상태)","categories":[],"tags":[{"name":"project","slug":"project","permalink":"https://hesthers.github.io/archives/tags/project/"}]},{"title":"preprocessing code","slug":"preprocessing-code","date":"2022-01-07T14:44:11.000Z","updated":"2022-01-07T14:51:45.561Z","comments":true,"path":"2022/01/07/preprocessing-code/","link":"","permalink":"https://hesthers.github.io/archives/2022/01/07/preprocessing-code/","excerpt":"","text":"Split columns in data1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586cols = &#123;&#x27;obj&#x27;: [], &#x27;cat&#x27;: [], &#x27;cont&#x27;: [] &#125;def data_split(): file_name = &#x27;final_data_v2.csv&#x27; file_path = os.getcwd()+&#x27;/drive/MyDrive/Colab Notebooks/&#x27; #[:-16]는 본인 경로에 맞게 있어도 되고 없어도 됨. df = pd.read_csv(file_path+file_name, encoding=&#x27;utf-8&#x27;) # 데이터 유형별 분류하기 for dt_idx, dt_val in zip(df.dtypes.index, df.dtypes.values): if &#x27;category&#x27; in dt_idx: df[[&#x27;category1&#x27;]] = LabelEncoder().fit_transform(df[[&#x27;category_id_1&#x27;]]) cols[&#x27;cat&#x27;].append(&#x27;category1&#x27;) if dt_val == &#x27;object&#x27;: if (&#x27;id&#x27; in dt_idx) | (&#x27;time&#x27; in dt_idx) | (&#x27;name&#x27; in dt_idx) | (&#x27;keyword&#x27; in dt_idx) |(&#x27;url&#x27; in dt_idx): df.drop(columns = dt_idx, axis=1, inplace=True) else: cols[&#x27;obj&#x27;].append(dt_idx) else: if (&#x27;id&#x27; in dt_idx) | (&#x27;time&#x27; in dt_idx): df.drop(columns = dt_idx, axis=1, inplace=True) else: if len(df[dt_idx].value_counts()) &lt;= 30: #연속형 데이터 중 30개 내의 범주로 나눌 수 있는 데이터 = category로 구분. cols[&#x27;cat&#x27;].append(dt_idx) else: if (&#x27;hour&#x27; in dt_idx) | (&#x27;group&#x27; in dt_idx): pass else: cols[&#x27;cont&#x27;].append(dt_idx) return cols# using splited columns, make new data framedef reorganization(df): data = pd.DataFrame() cols = data_split() for k, v in cols.items(): for v in cols[k]: if v in df.columns: data[v] = df[c] else: pass return data# preprocessing datadef preprocessing(data, cols): # 데이터 유형별 분류하기 data = reorganization(df) modified_df = pd.DataFrame() vec_dict = &#123;idx: [] for idx in range(len(data.columns))&#125; feature_index = [] for i, c in enumerate(data.columns): if c in cols[&#x27;obj&#x27;]: obj_data = pd.get_dummies(data[c], prefix=c, prefix_sep = &quot;/&quot;) modified_df = pd.concat([modified_df, obj_data], axis=1) vec_dict[i] = list(obj_data.columns) feature_index.extend(repeat(i, obj_data.shape[1])) elif c in cols[&#x27;cat&#x27;]: # click_label 컬럼 = y 변수로 사용 if &#x27;click&#x27; in c: pass else: cat_data = pd.get_dummies(data[c], prefix=c, prefix_sep = &quot;/&quot;) vec_dict[i] = list(cat_data.columns) feature_index.extend(repeat(i, cat_data.shape[1])) modified_df = pd.concat([modified_df, cat_data], axis=1) else: scaled_num_data = MinMaxScaler().fit_transform(df[v]) scaled_num_data = pd.DataFrame(scaled_num_data, columns = v) modified_df[v] = scaled_num_data vec_dict[i] = list(scaled_num_data.columns) feature_index.extend(repeat(i, scaled_num_data.shape[1])) print(&#x27;---- Data info ----&#x27;) print(cols) print(&#x27;Data Frame shape: &#123;&#125;&#x27;.format(data.shape)) print(&#x27;# of Feature: &#123;&#125;&#x27;.format(len(feature_index))) print(f&#x27;# of Field: &#123;len(vec_dict)&#125;&#x27;) print(f&#x27;Modified DF columns: &#123;data.columns&#125;&#x27;) return vec_dict, feature_index, modified_df","categories":[],"tags":[{"name":"project","slug":"project","permalink":"https://hesthers.github.io/archives/tags/project/"}]},{"title":"just my thinking","slug":"just-my-thinking","date":"2022-01-06T12:48:38.000Z","updated":"2022-01-06T12:58:02.543Z","comments":true,"path":"2022/01/06/just-my-thinking/","link":"","permalink":"https://hesthers.github.io/archives/2022/01/06/just-my-thinking/","excerpt":"","text":"매일 매일 프로젝트와 관련된 내용들이 주를 이루다보니 소재가 고갈되고 있는 기분이다. (나는 누구? 여긴 어디?… (?????)) 오늘 DeepFM 모델링하다가 전처리에서 코드가 자꾸 오류가 나고 진도도 나가지 못한채 막혀서 짜증이 났다. 논문 내용을 참고해서 다른 블로그 등에서 다른 사람들이 만들어 놓은 코드들을 가지고 활용해서 하다보니 시간도 오래 걸리고 이렇게 막혀버리니 답답하기도 하고… 제대로 하고 있는 것 같은데 왜 안되는지… 주말을 활용해서라도 마무리 해야지.. 나를 위한 것이라고 생각하면 지금 하는 것들이 고통스럽더라도 유의미한 일이 아닐까… 올해는 모든 것이 잘되기를… 내가 이루고자 하는 모든 것들을 이룰 수 있기를…","categories":[],"tags":[]},{"title":"what I did today","slug":"what-I-did-today-1","date":"2022-01-05T13:11:40.000Z","updated":"2022-01-05T13:21:35.854Z","comments":true,"path":"2022/01/05/what-I-did-today-1/","link":"","permalink":"https://hesthers.github.io/archives/2022/01/05/what-I-did-today-1/","excerpt":"","text":"I uploaded the most of posts regarding the project so far, so today’s post is about what I did today. 早上： did yoga 安排今天的计划 晚上： 运动（55分钟） 学习中文 （跟着说短的新闻报道和书写练习） did the project with a company: DeepFM modeling python codes 明天再见！！","categories":[],"tags":[]},{"title":"DeepFM implementation before codes","slug":"DeepFM-implementation-before-codes","date":"2022-01-04T14:07:33.000Z","updated":"2022-01-04T14:51:05.951Z","comments":true,"path":"2022/01/04/DeepFM-implementation-before-codes/","link":"","permalink":"https://hesthers.github.io/archives/2022/01/04/DeepFM-implementation-before-codes/","excerpt":"","text":"citation of a journal (originate from): Guo, H., TANG, R., Ye, Y., Li, Z., &amp;amp; He, X. (2017). DeepFM: A factorization-machine based neural network for CTR prediction. Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence. https://doi.org/10.24963/ijcai.2017/239 DeepFM modelingDeepFM의 구성: FM component + deep component $\\langle FM component \\rangle $ 피쳐 = i일 때, 스칼라 값 $w_i$ = order-1 중요도 가중치 잠재 벡터(latent vector) $V_i$ = 다른 피쳐와 상호작용 정도를 측정 $V_i$ (잠재 벡터): order-2 상호작용을 모델링을 위한 FM component &amp; 높은 차원의 피쳐 상호작용을 모델링을 위해 Deep Component에 포함. $w_i$ 와 $V_i$ &amp; 이 외의 포함한 모든 변수 -&gt; 결합된 예측 모델에서 학습. order-2 (FM 모델 쌍별 피쳐의 상호작용은 각각의 피쳐의 잠재 벡터들 간의 내적곱으로 되어 있음. &amp; 데이터 셋이 매우 sparse(드문 드문하게 퍼져있는 형태)일 때 훨씬 더 효과적으로 나타낼 수 있음. FM model: 피쳐 i가 데이터에 있을 때마다 잠재 벡터 $V_i$를 훈련시킬 수 있음. $\\langle formula \\rangle $ $\\hat{y}(x) := w_0 + \\sum^n_{i=1}w_i x_i + \\sum^n_{i=1}\\sum^n_{j=i+1} \\langle{v_i,v_j}\\rangle x_ix_j = \\langle{v_i,v_j}\\rangle + \\sum^n_{i=1}\\sum^n_{j=i+1} \\langle{v_i,v_j}\\rangle x_ix_j $ $\\langle{v_i,v_j}\\rangle := \\sum^k_{f=1}v_{i,f} \\cdot v_{j,f}$ $\\hat{y} = sigmoid(y_{FM} + y_{DNN})\\label{1}$ $\\langle{v_i,v_j}\\rangle$: order-1 피쳐들의 중요성을 반영. $\\sum^n_{i=1}\\sum^n_{j=i+1} \\langle{v_i,v_j}\\rangle x_ix_j $: order-2 피쳐 상호작용의 영향력을 나타냄.","categories":[],"tags":[{"name":"project","slug":"project","permalink":"https://hesthers.github.io/archives/tags/project/"}]},{"title":"preprocessing code for DeepFM modeling","slug":"preprocessing-code-for-DeepFM-modeling","date":"2022-01-03T12:08:28.000Z","updated":"2022-01-03T12:57:42.509Z","comments":true,"path":"2022/01/03/preprocessing-code-for-DeepFM-modeling/","link":"","permalink":"https://hesthers.github.io/archives/2022/01/03/preprocessing-code-for-DeepFM-modeling/","excerpt":"","text":"The following code is based on python: This code is for preprocessing code of DeepFM modeling to predict CTR. X feature of Deep FM model = various fields (nominal, continuous variables) with n-dimensional vector Y feature of Deep FM model = 0, 1 labels (whether the user clicked or not) nominal field =&gt; one-hot encoding vector continuous field =&gt; the value as it is, or discretization (change into categorical data) then one-hot vectorization 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# preprocessing codecols = &#123;&#x27;obj&#x27;: [], &#x27;cat&#x27;: [], &#x27;cont&#x27;: [] &#125;file_name = &#x27;final_data.csv&#x27;file_path = os.getcwd()[:-16]+&#x27;data\\\\&#x27; #[:-16]는 본인 경로에 맞게 있어도 되고 없어도 됨.df = pd.read_csv(file_path+file_name, encoding=&#x27;utf-8&#x27;)def preprocessing(df): # 데이터 유형별 분류하기 for dt_idx, dt_val in zip(df.dtypes.index, df.dtypes.values): if &#x27;category&#x27; in dt_idx: df[[&#x27;goods_cat&#x27;]] = LabelEncoder().fit_transform(df[[&#x27;category_id_1&#x27;]]) if dt_val == &#x27;object&#x27;: if (&#x27;id&#x27; in dt_idx) | (&#x27;time&#x27; in dt_idx) | (&#x27;name&#x27; in dt_idx) | (&#x27;keyword&#x27; in dt_idx) |(&#x27;url&#x27; in dt_idx): df.drop(columns = dt_idx, axis=1, inplace=True) else: cols[&#x27;obj&#x27;].append(dt_idx) else: if (&#x27;id&#x27; in dt_idx) | (&#x27;time&#x27; in dt_idx): df.drop(columns = dt_idx, axis=1, inplace=True) else: if len(df[dt_idx].value_counts()) &lt;= 5: #연속형 데이터 중 5개 내의 범주로 나눌 수 있는 데이터 = category로 구분. cols[&#x27;cat&#x27;].append(dt_idx) else: if (&#x27;hour&#x27; in dt_idx) | (&#x27;group&#x27; in dt_idx): pass else: cols[&#x27;cont&#x27;].append(dt_idx) for k, v in cols.items(): # 컬럼 전처리 (스케일링, 원핫인코딩) if k == &#x27;obj&#x27;: obj_data = pd.get_dummies(df[v], drop_first = True) df = pd.concat([df, obj_data], axis=1).drop(columns = v, axis=1) elif k == &#x27;cont&#x27;: num_data = RobustScaler().fit_transform(df[v]) num_data = pd.DataFrame(num_data, columns = v) df[v] = num_data print(&#x27;---- Data info ----&#x27;) print(&#x27;X shape: &#123;&#125;&#x27;.format(df.shape)) print(&#x27;# of Feature: &#123;&#125;&#x27;.format(len(df.columns))) return df df = preprocessing(df)df.head(3)","categories":[],"tags":[{"name":"project","slug":"project","permalink":"https://hesthers.github.io/archives/tags/project/"}]},{"title":"machine learning code","slug":"machine-learning-code","date":"2022-01-02T12:21:59.000Z","updated":"2022-01-02T12:50:53.979Z","comments":true,"path":"2022/01/02/machine-learning-code/","link":"","permalink":"https://hesthers.github.io/archives/2022/01/02/machine-learning-code/","excerpt":"","text":"Finding the outliers — isolation forest123456789101112131415161718192021222324# GridSearchCV(모델, param_grid, cv, scoring, n_jobs, verbose)from sklearn.ensemble import IsolationForestparam_grid = &#123; &#x27;n_estimators&#x27;:[50, 100, 150], &#x27;max_samples&#x27;:[50, 100, 150, 200], &#x27;contamination&#x27;:[float(0.004), float(0.1), float(0.5), &#x27;auto&#x27;]&#125;# 전처리를 거친 x_train으로 grid search cross validation 수행grid_search = GridSearchCV(IsolationForest(max_features=1.0, bootstrap=False, random_state=42,), param_grid=param_grid, cv=10, n_jobs=-1, verbose=0, scoring = &#x27;accuracy&#x27;)grid_search.fit(dataset)# 가장 좋은 성능을 보인 파라미터 조합의 estimator을 model에 저장model = grid_search.best_estimator_# dataset 데이터를 예측 및 성능 평가data_pred = model.predict(dataset)#이상치 값 개수-data_pred[data_pred == -1].sum() #1038개## 그리드 서치 결과# IsolationForest(contamination=0.004, max_samples=50, n_estimators=50, random_state=42) Filling with the values using KNN imputer123456789101112131415161718192021from sklearn.impute import KNNImputerparam_grid = [ &#123;&#x27;n_neighbors&#x27;: range(3, 11, 2)&#125;]imputer = KNNImputer(missing_values=np.nan, add_indicator=True)gs = GridSearchCV(imputer, param_grid = param_grid, cv=10, n_jobs = -1, scoring = &#x27;f1&#x27;)gs.fit(df)# 가장 좋은 성능을 보인 파라미터 조합의 estimator을 model에 저장model = gs.best_estimator_#f1 scoring일 때 최적 모델 값KNNImputer(add_indicator=True, n_neighbors=3)## imputer 결과(f1 score)# KNNImputer(add_indicator=True, n_neighbors=3)# f1 score일 때 결측값 채운 부분 확인하기df[(df.category_id_1 == 0)] 모델링을 할 때 데이터 양이 많으면 시간이 오래 걸림 (2~3시간 기본) 구글 코랩에서 GPU로 돌려도 마찬가지 (딥러닝이 아니면 GPU로 잘 안돌아감.)","categories":[],"tags":[{"name":"project","slug":"project","permalink":"https://hesthers.github.io/archives/tags/project/"}]},{"title":"new year plan","slug":"new-year-plan","date":"2022-01-01T12:10:21.000Z","updated":"2022-01-01T12:18:16.902Z","comments":true,"path":"2022/01/01/new-year-plan/","link":"","permalink":"https://hesthers.github.io/archives/2022/01/01/new-year-plan/","excerpt":"","text":"Happy New Year! 新年快乐！！New year 2022 has already come!! So, I will post new year plan (perhaps bucket list?? lolol) today. 2월에 HSK 6급 따기 지금 하고 있는 프로젝트 1월에 마무리 잘하기 취업!!!!! 뽀개기 (취뽀 성공 IT 가자!!) —- 올해 꼭꼭!! 중국어 말하기 (능숙하게 할 수 있게!!) 책 읽기 (올해는 10권 완독할 수 있도록)","categories":[],"tags":[]},{"title":"project report","slug":"project-report","date":"2021-12-30T13:01:07.000Z","updated":"2021-12-30T13:18:46.953Z","comments":true,"path":"2021/12/30/project-report/","link":"","permalink":"https://hesthers.github.io/archives/2021/12/30/project-report/","excerpt":"","text":"2 days left until this year ends!!! Things that I did today figuring out the overall outliers in the dataset using iForest(Isolation Forest) dropping the outliers in the dataset after merging outlier dataframe to the first dataset result total outliers that the first dataset has: 1038 (anomaly detection) Things to do tomorrow Studying K-Means and KNN imputer to fill out the missing value in the dataset at least before noon","categories":[],"tags":[{"name":"project","slug":"project","permalink":"https://hesthers.github.io/archives/tags/project/"}]},{"title":"project process with feedback","slug":"project-process-with-feedback","date":"2021-12-29T11:37:06.000Z","updated":"2021-12-29T14:58:26.917Z","comments":true,"path":"2021/12/29/project-process-with-feedback/","link":"","permalink":"https://hesthers.github.io/archives/2021/12/29/project-process-with-feedback/","excerpt":"","text":"Q n A with feedbackQ1. 클릭 시간의 경우 데이터의 결측치가 96~97%인데 컬럼을 아예 제외하는 것이 나은지 아니면 해당 결측치를 다른 값으로 대체해서 채우는 것이 나은가? view_log, impression_log을 가지고 계산한 click_time을 어떻게 처리해야 할지..?? click_time ⇒ 음수값이 나오면 안되고 서버 기록 시간인지 해당 view, impression이 발생한 시간인지의 여부의 차이 (데이터의 정의 차이)문제. 정의를 고민해볼 문제.. (impression이 먼저 → view가 후행) 확실하게 하려면 EDA를 해보는 것이 좋음. (해당 click_time의 weight가 얼마나 되는지.. 클릭하는 데 있어서 과연 시간이 무관한지를 판단) 몇 초이상 노출이 되는 경우 클릭할 확률이 높다 (특정 시간 분포를 확인해보기) Q2. emergency_cnt 이상치 부분 처리를 어떻게 해야하는가 (삭제 or 스케일링 처리) 특정 값만 out 되어있는 경우 isolation forest 모델을 통해 여러 피쳐들 간의 영향력(여러 피쳐들간의 outlier 값들을 확인)을 파악할 수 있음. 눈에 띄는 해당 건수의 경우 outlier 값이 맞으나 해당 광고주에 대해 유의미한 정보는 아닐 수 있으나 학습에 사용할 지 말지는 따로 확인 필요함. 결측치나 이상값의 수가 상당히 적은 경우 주관적인 입장에서 아예 drop하는 것이 나음. Q3. 나이는 0세를 임의의 값으로 대체할지 아니면 따로 처리가 필요한가..? 입력을 안했거나 잘못 입력한 경우 (잘못된 데이터일 확률이 높음) 일반적으로 평균값 혹은 다른 특징의 데이터들을 가지고 나이 추정함. 데이터를 수집하는 과정에서 데이터 수집하기 어려운 경우가 존재→ 데이터를 버리지 않을 때 나이, 성별을 제외한 나머지 피쳐들을 유사도를 계산해서 가까운 인원 수의 평균값 계산 (KNN imputer) 방식 사용. 성별 알 수 없음은 로그인 안해서 알 수 없음으로 판단. (의도를 가지고 응답을 하지 않는 경우라고 보기 어려움.) 비율이 치중되지 않는 이상 결측치 처리 시 KNN imputer or 버리기 (결측값이 많을 때에는 데이터를 버리는 것은 pass) 추가 피드백 많은 알고리즘이 추천 시스템에 들어가 복잡함. cold start 문제가 추천 시스템 구현이 어려운 이유임. 로그 데이터로 추천 문제를 해결하는 것이 어려움. 데이터가 하루치 이므로 결론이 애매할 수 있음. (데이터 문제임) 지금까지의 모든 프로젝트 준비 과정들이 의미있음. project process 논의 후 click_time 계산 관련된 논란 종결!! 나이/성별 데이터 확인하고 결측치 및 이상값 확인 필요함. (KNN Imputer &amp; Isolation Forest 방법 사용해서 전반적인 이상값의 경향 확인하기)","categories":[],"tags":[{"name":"project","slug":"project","permalink":"https://hesthers.github.io/archives/tags/project/"}]},{"title":"Project meeting note","slug":"Project-meeting-note-0","date":"2021-12-28T09:04:38.000Z","updated":"2021-12-28T10:42:12.089Z","comments":true,"path":"2021/12/28/Project-meeting-note-0/","link":"","permalink":"https://hesthers.github.io/archives/2021/12/28/Project-meeting-note-0/","excerpt":"","text":"오전 논의 사항 emergency_cnt 이상치를 삭제해도 무방해보임. adver_follower_count 컬럼 삭제 8% 정도 비율의 데이터들을 완전히 삭제할지 여부 3~4% 상품이 노출되어 클릭까지 진행한 고객 제외 후 나머지 96%는 단순히 노출되어 서버에 접속한 경우의 결측치 처리 여부 오후 논의 사항 imp_id 중복제거한 데이터 셋을 다시 생성 후 전처리 (오늘 할 일!!) 데이터 전처리 방법 확인하기 이상치 삭제 후 데이터 판단 결측치, 이상치 처리 및 질문 사항 논의 (약간의 영향력은 남겨두되 최대한 이상치, 결측치 처리하기) 결측치가 8%정도 되는 데이터 컬럼들 삭제 click_time 컬럼은 절댓값을 씌운 후 다시 컬럼 생성하여 이후 단계 진행 (양수로 만들기) emergency_cnt 이상치 부분 처리 문제 질문. flag_used 카테고리 질문 나이는 범주화 (0세를 임의의 값으로 대체할지 아니면 따로 처리가 필요한지 질문) click_time 결측치 처리 질문 데이터 전처리결측치 처리 판단 device_type_x 컬럼의 경우 데이터 값이 전부 a이므로 무의미한 데이터로 보여짐. adver_follower_count 컬럼 또한 0이라는 데이터 값만 가지고 있으므로 무의미한 데이터. → 처리 방법: 삭제 click_time의 경우 - (음수) 값이 존재. (이 부분과 결측치 처리 문제가 발생) 음수값이 존재하는 이유: impression의 서버 접속 시간이 view 서버 접속시간보다 늦기 때문.. 결측치 값의 비중이 96~7%이므로 삭제는 안됨… 3~4% 상품이 노출되어 클릭까지 진행한 고객 제외 후 나머지 96%는 단순히 노출되어 서버에 접속한 경우→ 처리 방법: 무조건적인 삭제는 금물.. 절댓값을 씌운 상태에서 click_time을 새로 계산. 수치형 데이터 중 이상치 처리 필요해 보이는 컬럼들: → bid_price_x, emergency_cnt, commen_cnt, user_age(나이는 임의로 0을 넣은 건가…..), adver_pay_count, adver_parcel_post_count, adver_transfer_count, adver_chat_count 등… 눈에 띄는 이상값들은 비율 확인해서 삭제하기 스케일링 진행할 때 robust_scaler로 진행. name, keyword의 경우 텍스트 분석 진행 가능 (카카오 아레나 블로그 참고해서 태그 별 분석도 가능해보임)","categories":[],"tags":[{"name":"project","slug":"project","permalink":"https://hesthers.github.io/archives/tags/project/"}]},{"title":"project process for new week","slug":"project-process-for-new-week","date":"2021-12-27T13:50:59.000Z","updated":"2021-12-27T14:05:40.812Z","comments":true,"path":"2021/12/27/project-process-for-new-week/","link":"","permalink":"https://hesthers.github.io/archives/2021/12/27/project-process-for-new-week/","excerpt":"","text":"Project Process for the new week Week 3 Finished studying the deep learning(DL) modeling This week: begin the preprocessing Table join Preprocessing - data type, outlier, missing value","categories":[],"tags":[{"name":"project","slug":"project","permalink":"https://hesthers.github.io/archives/tags/project/"}]},{"title":"project process week1 end","slug":"project-process-week1-end","date":"2021-12-24T14:25:16.000Z","updated":"2021-12-24T14:29:55.515Z","comments":true,"path":"2021/12/24/project-process-week1-end/","link":"","permalink":"https://hesthers.github.io/archives/2021/12/24/project-process-week1-end/","excerpt":"","text":"The process of a project with a company’s data 피드백: 이대로 쭉 진행하기 (여러 논문들 참고 및 코드들 돌려보기)추가 학회 논문 참고오토인코더도 추천시스템에 자주 쓰이는 모델. 카카오 블로그 등 참고해서 공부하고 확인할 수 있음. Merry Christmas!!","categories":[],"tags":[{"name":"project","slug":"project","permalink":"https://hesthers.github.io/archives/tags/project/"}]},{"title":"project process before feedback","slug":"project-process-before-feedback","date":"2021-12-23T13:27:15.000Z","updated":"2021-12-23T13:29:18.624Z","comments":true,"path":"2021/12/23/project-process-before-feedback/","link":"","permalink":"https://hesthers.github.io/archives/2021/12/23/project-process-before-feedback/","excerpt":"","text":"Deep Learning Project autoencoder modeling study (required) finished DeepFM modeling using wine and breast cancer dataset from sklearn datasets","categories":[],"tags":[{"name":"project","slug":"project","permalink":"https://hesthers.github.io/archives/tags/project/"}]},{"title":"project progress","slug":"project-progress","date":"2021-12-22T14:31:50.000Z","updated":"2021-12-22T14:38:22.829Z","comments":true,"path":"2021/12/22/project-progress/","link":"","permalink":"https://hesthers.github.io/archives/2021/12/22/project-progress/","excerpt":"","text":"Project Progress 어려움/ 문제점 코드 리뷰하면서 해당 데이터 셋 제외한 나머지 데이터 셋으로는 코드가 먹히지 않음. 해당 DeepFM 모델 코드 실행시 오류가 발생. 이후 예상되는 어려움 DeepFM 모델에 대하여 데이터 셋에 적절한 코드를 작성 (오랜 시간이 필요할 것으로 보여짐.)기타 추가 모델이 필요하므로 이에 대한 추가 코드도 작성하면서 시간이 요구될 것으로 보여짐.딥러닝 모델 코드 구축하기 위한 별도의 공부/학습이 필요함.","categories":[],"tags":[{"name":"project","slug":"project","permalink":"https://hesthers.github.io/archives/tags/project/"}]},{"title":"project meeting note","slug":"project-meeting-note","date":"2021-12-21T12:56:09.000Z","updated":"2021-12-21T13:45:11.995Z","comments":true,"path":"2021/12/21/project-meeting-note/","link":"","permalink":"https://hesthers.github.io/archives/2021/12/21/project-meeting-note/","excerpt":"","text":"Today’s project meeting note journal/article review with code review DeepFM model-based modeling Tomorrow: do code execution with code review using different datasets","categories":[],"tags":[]},{"title":"DeepFM Journal Review","slug":"DeepFM-Journal-Review","date":"2021-12-20T09:16:37.000Z","updated":"2021-12-20T12:06:50.803Z","comments":true,"path":"2021/12/20/DeepFM-Journal-Review/","link":"","permalink":"https://hesthers.github.io/archives/2021/12/20/DeepFM-Journal-Review/","excerpt":"","text":"Journal Review about DeepFM model (pt.1)DeepFM 모델과 CTR 예측에 관한 논문 하나를 정리해보려고 한다. (사실 리뷰라고 쓰고 정리라 읽는 그런 포스팅이다.)프로젝트 준비하면서 DeepFM 관련 논문들을 읽어봤지만 사실 무슨 말인지도 모르겠고 과연 코드로 구현할 수 있을지 의문이 들기는 하지만 요약해서 정리해볼까 싶다. Title: A New Approach for Advertising CTR Prediction Based on Deep Neural Network via Attention Mechanism (Based on Wang, Q., Liu, F., Xing, S., &amp; Zhao, X. (2018). A New Approach for Advertising CTR Prediction Based on Deep Neural Network via Attention Mechanism. Computational &amp; Mathematical Methods in Medicine, 1–11. https://doi-org.libproxy.library.unt.edu/10.1155/2018/8056541) Introduction: CTR(Click-Through Rate) is critical element to many recommender systems and advertising as estimating the ratio of clicks to impressions of ads, that is to say, how many ads would be displayed to consumers. The Internet market and electronic commerce industry has been rapidly developing, so the online ads with the media are also key issue. As following those development, CTR is an critical indicator to measure the effectiveness of advertising display. Deep learning fields have honored the achievements of NLP, image data processing, so in this article, the authors introduce various methods of CTR prediction based on Deep FM (a part of deep learning) models. Main points (part 3. — This is the main section that I want to emphasize on this post!) Click-through Rate Estimation Based on Deep Neural Network (p.g. 3 to 7) To mine highly correlated features, reducing high spareness of features of data, or data dimensional reduction is needed. The complex relationships between various, different types of objects are in data and there is also the similiarity between the same types of objects, so the dimensionality reduction is required. In this article, using K-Means clustering algorithm to group/cluster the similar objects into the same cluster for click log data, calculated the distance between user-query-ad (3D vector). I intentionally omitted the formula in this post. (Regarding formula, I will post on another posting. (To be continued) I used this article for personal study and personal project, not the commercial purpose.","categories":[],"tags":[{"name":"journal review","slug":"journal-review","permalink":"https://hesthers.github.io/archives/tags/journal-review/"}]},{"title":"what I did today","slug":"what-I-did-today-0","date":"2021-12-19T13:58:17.000Z","updated":"2021-12-19T14:23:04.448Z","comments":true,"path":"2021/12/19/what-I-did-today-0/","link":"","permalink":"https://hesthers.github.io/archives/2021/12/19/what-I-did-today-0/","excerpt":"","text":"What I studied/did today (TIS) Read journals about DeepFM Studied Chinese Updated things related to prepraring for careers (Resume) 내일 있는 커리어 상담 질문 준비 면접 시 자기소개하는 질문에 대한 답변을 어떻게 시작하는 것이 좋은가 면접 예상질문들 중 인성면접에서 원하는 답변의 기준은 무엇인가 자기소개서에서 직무동기에 대한 답변을 할 때 특정 경험이 없는 경우 어떻게 표현하는 것이 좋은가… 인성면접에서 자신의 장/단점을 어떻게 어필해야 할까 취미나 특기에 대해서 물어보면 면접관에게 직무와 관련된 취미나 특기를 이야기하는 것이 좋은가.. 아니면 솔직하게 있는 그대로 이야기하는 것이 좋은가 (너무 당연한 답을 얘기해야하는 것이 맞나) 포트폴리오의 경우 이미 프로젝트 시 만들었던 결과물을 취합해서 ppt 형식으로 만들어두는 것이 좋은가 아니면 워드 파일에 정리해두는 것이 좋은가 (특정 형식에 맞춰 정리???) 그 밖의 예상질문들 추가 찾아보고 생각해서 준비하기","categories":[],"tags":[]},{"title":"New project with a company","slug":"Machine-Learning-project-1","date":"2021-12-16T11:31:44.000Z","updated":"2021-12-16T11:43:14.915Z","comments":true,"path":"2021/12/16/Machine-Learning-project-1/","link":"","permalink":"https://hesthers.github.io/archives/2021/12/16/Machine-Learning-project-1/","excerpt":"","text":"기업 프로젝트 purpose: to improve CTR (Click Through Rate) the result of this project: algorithm for recommending ads with high CTR data: 번개장터 데이터 (광고, 광고주 데이터, 상품 노출 및 클릭 여부 데이터) 프로젝트 진행 시 활용할 자료들(References) Kaggle &amp; Github: machine learning technique codes blogs journals: about concepts and theories of various machine learning techniques models Deep FM (a part of NN) Ensemble, Clustering, Naive Bayes to classify the group of customers to provide the recommendation service LDA and CNN to analyze the text and image data","categories":[],"tags":[]},{"title":"my idea for new project","slug":"my-idea-for-new-project","date":"2021-12-15T12:23:50.000Z","updated":"2021-12-15T12:32:00.642Z","comments":true,"path":"2021/12/15/my-idea-for-new-project/","link":"","permalink":"https://hesthers.github.io/archives/2021/12/15/my-idea-for-new-project/","excerpt":"","text":"New project — my idea 문제 정의 머신러닝(지도학습/비지도학습): 클러스터링, 분류 모델 주 사용 (KNN, Ensemble Classifier 등…), 연관 분석 기법, GBDT 딥러닝: CIFAR10 이미지 분석 문제: 고객 관점? vs. 서비스 제공자 관점? 상품 - 어떤 도메인인지 협업 / 내용기반 필터링 기법: 고객 카테고리에 맞는 아이템 추천 (클릭수, 구매수, 성별, 나이, 팔로잉 수 등..을 활용한 예측 기법 사용) 광고주 데이터를 통한 연관 상품 추천 고객 군집 분석(clustering) - 특정 고객집단의 특성 추출 및 파악 키워드와 제품명을 기반으로 한 텍스트 분석을 통한 추천시스템 추가: 개인화 상품 추천, 상품 브랜드 자동 추출, 연관 검색어 개발, 거래 위험 (Fraud 위험 탐지 알고리즘) 데이터 정의 및 수집 데이터 셋: ad / advertiser / view_log / impression_log / viewer // dataset 사용할 컬럼들: 상품 아이디, 상품 입찰가, 키워드, 나이, 성별, 로그 데이터 시간대 등… (device type, 광고 상품 신고수 컬럼 은 제외) 테이블 조인 (SQL 활용), table merge, 이미지 크롤링 혹은 sql 쿼리문 활용, 키워드를 통한 텍스트 분석, user-item matrix (거리 기반 유사도), … URL 크롤링으로 이미지 저장 후 텍스트 분석 및 이미지 분석으로 추천 상품 제공 (가능하다면 번개 장터 홈페이지 후기 등 크롤링으로 긍부정 분석) EDA: 파이썬과 태블로 활용 최종 결과물을 UI/UX로 Figma를 활용해서 구현해보기 (번개 장터 추천시스템 앱을 만든다면??) 해당 문제에 대한 기존 방법론 조사 논문: 추천시스템 관련 내용들 (예시 논문: 딥러닝 기반 나이 예측과 자연어처리를 활용한 의류추천 시스템) 캐글 사이트: 추천시스템 모델 코드들에 관한 참고 레퍼런스 github에서 관련 모델 코드들 참고 온라인 강의(The Red): 추천시스템에 관한 실습코드를 레퍼런스용으로 사용 사용할 협업 툴(환경): 슬랙: 멤버, 강사님과의사소통 github: 프로젝트 코드 공유 notion: 보고서 및 업무 분담 등 기록 수행 환경: Google Colab or Jupyter Notebook SQL Tableau Figma 참고기획서에 반드시 포함되어야 하는 내용들 문제 정의 데이터 정의 해당 문제에 대한 기존 방법론 조사 (논문, tech review 등) 사용할 모델 리스트 (rough하게) 프로젝트를 수행할 환경 (Google Colab, Kaggle Notebook, AWS 등)추가 사항: 프로젝트 계획 (rough하게)","categories":[],"tags":[]},{"title":"Deep Learning","slug":"Deep-Learning","date":"2021-12-14T12:42:47.000Z","updated":"2021-12-14T13:28:14.275Z","comments":true,"path":"2021/12/14/Deep-Learning/","link":"","permalink":"https://hesthers.github.io/archives/2021/12/14/Deep-Learning/","excerpt":"","text":"Deep Learning Framework TensorFlow PyTorch From Google From Meta (Old: FB / FaceBook) good at hardware optimization good for research or study a set of Keras and with TPU on CoLab easy to debug and high rate of opensource Difference between ML and DL Whether the feature extraction/engineering is with classification in the modeling or notDoes Human expert do the feature engineering or not?? CNN a kind of deep learning fields it is for image classification. Using convolutional calculation, pooling, ReLU in the process Convolutional Layer a convolutional filter or convolutional filters(=kernels) is/are on the input image. when this filter moves next or under, it is called as the stride. convolutional calculation: multiply the filters of input image with the weight(random value at first) the size of activation maps: (input width - filter width) / stride + 1 channel(depth) of activation maps = how many filters are","categories":[],"tags":[]},{"title":"New Project with a company","slug":"New-Project-with-a-company","date":"2021-12-13T14:18:05.000Z","updated":"2021-12-13T14:29:26.638Z","comments":true,"path":"2021/12/13/New-Project-with-a-company/","link":"","permalink":"https://hesthers.github.io/archives/2021/12/13/New-Project-with-a-company/","excerpt":"","text":"New ProjectData for new project 광고와 광고 노출여부 확인 데이터 하루치 데이터밖에 없으므로 결과해석에서는 한계가 있으나 분석에 대한 인사이트를 얻을 수 있음. final classification (0,1로구분하여 확률을 머신러닝으로 계산하기) 문제 정의를 명확하게 해줘야 함.!!!!!!! (이번 주 동안에) 머신러닝 추천문제로 어떤 문제를 해결할 것인가 feature vector, 유사도 등 어떻게 구성할 것인가.. 문제를 얼마나, 어떻게 구현할 것인가 Guidelines for a project 머신러닝 문제 정의 — 분류, 클러스터링 등 머신러닝 기법 사용 데이터 정의 (어떤 컬럼이 있는지, 비정형데이터의 경우 size가 어떻게 되는지…) 분석에 사용할 데이터 정의 문제 정의가 우선되어야 함. (테이블 join, 어떤 feature vectors, user-item matrix) 환경세팅 (github, slack, notion 등 활용) 협업 (발표 보고서 - 노션 활용), 코드 공유 환경(github) 어떤 문제를 어떻게 해결할 것인지 기획서!! 문제 정의 - 데이터 정의 - 기존 방법론 (문제에 대한) - 모델리스트 - 수행 환경.. My idea 이미지 url을 이용해서 크롤링하고 이미지 분석 후 추천 시스템.. 광고 데이터에서 제품 별 컨텐츠 주 단어를 활용하여 텍스트 분석을 통한 추천 시스템 카테고리별 (id별, 시간별, 제품별) 등으로 추천시스템 예측을 통한 추천시스템 (카테고리, 이용시간대, 성별 등)","categories":[],"tags":[]},{"title":"Just note","slug":"Just-note","date":"2021-12-12T14:10:23.000Z","updated":"2021-12-12T14:20:03.685Z","comments":true,"path":"2021/12/12/Just-note/","link":"","permalink":"https://hesthers.github.io/archives/2021/12/12/Just-note/","excerpt":"","text":"What I watched yesterday (movie) Title: A man called Ove This movie was impressive. This movie shows life of a man called Ove, is not much kind man to his neighbors. However, after meeting the new neighbor family, he changes a lot. His life is also changed. He realizes what the better and beautiful life is. 삶이란 아름답다. 인생에서 소소한 행복을 느끼는 것. 그것이 진짜 나의 행복이라 할 수 있을 것이다. 누구나 저마다의 사연이 있고 그 사연에서 숨겨진 이야기가 나를 때로는 변화시키기도 한다. 사람이 사람에게 상처를 주기도 하지만 사람으로부터 치유를 받는다. 누구나 받은 그 상처를 사람으로 하여금 치유받으며 그 속에서 기쁨과 행복을 느끼는 것은 아닐까…그래서 이 영화를 추천한다.","categories":[],"tags":[]},{"title":"Machine Learning Project with the end","slug":"Machine-Learning-Project-with-the-end","date":"2021-12-11T14:14:16.000Z","updated":"2021-12-11T14:23:50.907Z","comments":true,"path":"2021/12/11/Machine-Learning-Project-with-the-end/","link":"","permalink":"https://hesthers.github.io/archives/2021/12/11/Machine-Learning-Project-with-the-end/","excerpt":"","text":"Machine Learning Project (END!!!) with feedback드디어 어제부로 머신러닝 그룹 프로젝트가 끝이 났다.. 피드백과 함께 프로젝트를 앞으로 할 때 어떠한 부분들이 필요할 지를 정리했다. 질문에 대하여 연쇄 작용이 일어날 수 있도록 해주는 것이 좋음 (청중들이 계속해서 다음 내용을 궁금해 하고 다음에 뭐가 나올지 바로 판단할 수 있도록 ppt를 구성하는 것이 좋다) 너무 많은 내용을 한 페이지에 담지 않게 해당 페이지의 내용이 명확하게 눈에 딱 들어올 수 있도록.. EDA라면 raw data에 대한 분석을 목적으로 하는 것이기 때문에 이 부분에 초점을 두고 진행해야 함. 단순히 시각화만 표현한다고 되는 것이 아님. 시각화에 담겨있는 내용이 무엇인지 나타낼 수 있어야 함. 플로우 진행사항이 보일 수 있게 ppt 진행사항이나 방향을 명확하게 해둘 필요가 있음. 서론 부분이 너무 길지 않게 표현하기 (배경) 내용의 핵심이 무엇인지… (발표의 방향이 흩어져 보이지 않도록 주의하기 = 플로우의 방향이 유지될 수 있도록 하기) 직접 수집한 데이터를 사용한 점 (크롤링 등의 노동량을 투입한 것)은 좋았으나 발표 시 표현하지 않았다는 점에서 아쉬움. 배우지 않은 모델을 적용했다는 점도 좋았음. 앞으로의 프로젝트 시 확인해야하는 부분들 데이터를 가져와서 그 안의 파생 변수를 만들어본 시도가 필요함. GUI 환경에서 구현하여 실행해보는 시도가 필요함. 데이터 전처리와 모델링에서 많은 노력을 투입하는 것이 필요함 (체계적으로 수행하는 것이 필요함.) 새로운 파생변수를 추가하면서 비교해나가는 것이 필요함. 최적의 파라미터를 통해 가장 좋은 파라미터를 가진 모델들끼리 비교하고 최적의 결과를 내는 그 과정이 의미있으므로 필요함. 수상작 그대로 clone하는 것이 프로젝트가 가치가 있는가는 생각하지 말기 그대로 레퍼런스를 참고할 수 없는 경우도 존재함. 저품질 사양의 컴퓨터로 한 프로젝트를 통하여 만든 질이 낮은 성과물을 무조건으로 나쁘다고 볼 수 없음 레퍼런스를 스터디하고 어떻게 카피할 것인지가 중요함. 개선해야 하는 부분들을 공부하고 찾아나가야 함.","categories":[],"tags":[]},{"title":"machine learning project with feedback 4","slug":"machine-learning-project-with-feedback-4","date":"2021-12-08T09:22:26.000Z","updated":"2021-12-08T10:44:43.077Z","comments":true,"path":"2021/12/08/machine-learning-project-with-feedback-4/","link":"","permalink":"https://hesthers.github.io/archives/2021/12/08/machine-learning-project-with-feedback-4/","excerpt":"","text":"Machine Learning Project Process with 4th feedback 관광지와 상권은 분리할 것. 상권밀집지수는 관광지와 하나로 보고 계산한 것이 잘못된 방향. 관광지와 상권은 다른 분류로 구분해야함. 지수 선정에 대한 명확한 근거가 필요함. project 관련된 필요 사항들 프로젝트 시 해결하고자 하는 문제들(정의한 문제들)과 그에 따른 해결방안 제시할 것. (설득력이 있어야 함.) 상대방의 공감을 살 수 있어야하고 대중적인 문제를 가지고 프로젝트하는 것이 좋음. 명확하고 분명한 해당 그룹만의 방향과 색깔을 드러내줄 수 있어야 함. 내용전달을 명확하게 해야하고 프로젝트에 대한 이해가 우선시 되어야 함. 기대효과를 마지막에 나타내줄 필요가 있음. (문제 정의 및 제시한 문제들에 연관지어야 함.) 발표자가 발표하는 데 있어서 주도권을 가지고 있으므로 질문을 유도할 수 있도록 해야함. 청중들의 관심을 계속해서 가질 수 있도록 해줄 필요가 있음. (위의 사항들은 특강을 듣고 난 다음 간단하게 정리해본 내용으로 불펌 금지합니다.)","categories":[],"tags":[]},{"title":"What I did today","slug":"What-I-did-today","date":"2021-12-07T13:54:01.000Z","updated":"2021-12-07T14:14:07.719Z","comments":true,"path":"2021/12/07/What-I-did-today/","link":"","permalink":"https://hesthers.github.io/archives/2021/12/07/What-I-did-today/","excerpt":"","text":"Things to do today 입지 선정지수 선형회귀로 학습해서 찾아보기 (결과: 컬럼 간의 상관관계 문제로 인해 상관계수가 상당히 낮게 나옴.. 통계상으로 볼 때 여러 문제점들이 보여짐. 컬럼들 간의 상관관계 또한 낮음.) 중국어 (빅분기 시험이 끝났으니 HSK 6급 준비 다시 시작)","categories":[],"tags":[]},{"title":"Machine Learning Project Process","slug":"Machine-Learning-Project-Process","date":"2021-12-06T14:16:11.000Z","updated":"2021-12-06T14:54:55.083Z","comments":true,"path":"2021/12/06/Machine-Learning-Project-Process/","link":"","permalink":"https://hesthers.github.io/archives/2021/12/06/Machine-Learning-Project-Process/","excerpt":"","text":"machine learning project process Finished EDA and preprocessing related to accomodation data Began improving the index regarding developable area for EV station Things to do alread found saving geojson file problem =&gt; made geojson file merge two dataframe and began making the new score for machine learning","categories":[],"tags":[]},{"title":"Machine Learning Project week2","slug":"Machine-Learning-Project-week2","date":"2021-12-05T14:03:37.000Z","updated":"2021-12-05T14:48:19.776Z","comments":true,"path":"2021/12/05/Machine-Learning-Project-week2/","link":"","permalink":"https://hesthers.github.io/archives/2021/12/05/Machine-Learning-Project-week2/","excerpt":"","text":"데이터 분석 결과 숙박에 대한 별점과 게시확인 수의 상관관계는 보이지 않음 (수치상 -0.15로 나옴 스캐터 플롯을 그려보면 상관관계가 없어보임) 별점은 대체로 9.4에서 10점 이 쪽으로 몰려있으며 전반적으로 9점이상인 점수에 몰려있는 모양을 하고 있다. 가중치 범위로 정할 수 있는 피벗 테이블을 만들어 보면 4가지로 구분될 수 있음 (추천해요 부분이 대체로 많은 편) 게시확인수는 편차가 큰 편 그래서 MinMaxScaler를 통해 스케일링을 한 번 해주고 가중치 계산을 해줄 필요가 있음. (혹은 RobustScaler를 이용하거나) 숙박업소에 대한 카테고리별 추천도 여부는 큰 차이는 없어보이나 대체로 호텔/콘도를 더 많이 좋게 추천하는 편. 호텔/콘도의 경우 대기업에서 운영하는 경우가 많아서 입지도 넗고 상가업소 번호도 많음. 생각해볼 문제 이상치로 보여지는 데이터들이 있어보이나 과연 이 데이터들을 삭제하거나 고려사항에서 배제하는 것이 맞는가 별점에서 보여지는 편차들을 어떻게 고려해서 가중치를 부여할 것인가 호텔/콘도 쪽 추천도가 높은 이유는 여행 시 대체로 호텔이나 콘도를 많이 선택하며 호캉스 등의 여부로 선택을 하는 것은 아닌가.. 호텔/콘도에 대한 추천도가 높다는 것은 많이 이용했다는 것.. 사람들이 이용했을 때 좋았고 앞으로도 자주 이용할 것. (뇌피셜) 숙박업소에 경우 평가부분에서 편차가 큰 편.. 기존 추천도의 경우 구간이 4개의 구간으로 나누어지기 때문에 가중치에 대한 구간도 4개로 나누면 되지 않을까","categories":[],"tags":[]},{"title":"After BigData Certificate Exam","slug":"After-BigData-Certificate-Exam","date":"2021-12-04T14:02:20.000Z","updated":"2021-12-04T14:37:28.051Z","comments":true,"path":"2021/12/04/After-BigData-Certificate-Exam/","link":"","permalink":"https://hesthers.github.io/archives/2021/12/04/After-BigData-Certificate-Exam/","excerpt":"","text":"빅분기 실기 시험 후오늘 빅분기 실기 시험을 봤다.. 일단은 나쁘지 않았다. 전체적으로 충분히 도전해볼만한 문제들이였던 것 같다. 물론 단답형이랑 작업형 1유형 1번 문제가 고민을 좀 많이 했던 문제였다. 작업형 2유형 문제는 분류 문제였고 머신러닝으로 xgboost, 랜덤 포레스트, 그래디언트 부스팅, 로지스틱 회귀 모형을 썼는데 그래디언트 부스팅이 성능이 가장 좋게 나와서 최종 예측결과를 이걸로 사용해서 답안을 제출했다. 결측치가 전혀 없는 깔끔한 데이터여서 의외로 뭐지? 싶었던 것 같다. 스케일링은 수치형 데이터는 robust scaler, 명목형 데이터는 원핫인코딩으로 해서 전처리 해줬다. 단답형 1번은 어제 전날 밤에 봤던 문제였다. 향상도.. 나오겠다 싶어서 우상향 이렇게 외웠는데 1번에서 바로 나왔다.. 인공신경망 가중치 편향을 포함한 계산 문제는 0.13?으로 적은 것 같다.. 내 기억엔.. 작업형 1유형 1번 문제는 이미 틀린 것 같고 단답형에서도 점수가 좀 깎일 것 같다.. 합격하고 싶다 ㅠㅠㅠ (12/31 발표 실화??? 불합격하면 새해를 우울하게 보내라는 건가요….) 보면서 나쁘지 않다라고 느꼈는데 과연 점수가 괜찮을런지…","categories":[],"tags":[]},{"title":"Bigdata Machine Learning example","slug":"Bigdata-Machine-Learning-example","date":"2021-12-02T13:41:18.000Z","updated":"2021-12-02T13:45:45.557Z","comments":true,"path":"2021/12/02/Bigdata-Machine-Learning-example/","link":"","permalink":"https://hesthers.github.io/archives/2021/12/02/Bigdata-Machine-Learning-example/","excerpt":"","text":"Machine Learning Example한국데이터산업진흥원(KDATA/dataq.or.kr)에서 제시해준 예시 문제 답안 만들어봤다. 머신러닝 연습하면서 파이썬 코드 작성했다. (분류문제) xgboost, lightgbm은 시험환경에서 먹히지 않아서 패쓰… 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104import pandas as pdpd.set_option(&#x27;display.max_columns&#x27;, None)X_test = pd.read_csv(&quot;data/X_test.csv&quot;)X_train = pd.read_csv(&quot;data/X_train.csv&quot;)y_train = pd.read_csv(&quot;data/y_train.csv&quot;)# 사용자 코딩## 결측값 확인하기# print(X_test.isnull().sum()/len(X_test)) #1611개 결측값 존재# print(X_train.isnull().sum()) #2295개 결측값 존재# print(y_train.isnull().sum()) #0개 결측값 존재## 결측값 처리하기# print(X_test.describe()[&#x27;환불금액&#x27;])# print(X_train.describe()[&#x27;환불금액&#x27;])# print(X_test.corr()[&#x27;환불금액&#x27;])# print(X_train.corr()[&#x27;환불금액&#x27;])# X_test.fillna(0, inplace=True)# X_train.fillna(0, inplace=True)# print(X_test.describe()[&#x27;환불금액&#x27;])# print(X_train.describe()[&#x27;환불금액&#x27;])from sklearn.impute import KNNImputerX_test[&#x27;환불금액&#x27;] = KNNImputer(n_neighbors=5).fit_transform(X_test[[&#x27;환불금액&#x27;]])X_train[&#x27;환불금액&#x27;] =KNNImputer(n_neighbors=5).fit_transform(X_train[[&#x27;환불금액&#x27;]])# print(X_test.isnull().sum())# print(X_train.isnull().sum())# print(X_test.describe()[&#x27;환불금액&#x27;])# print(X_train.describe()[&#x27;환불금액&#x27;])## 데이터 유형 확인하기# print(X_test.info())# print(X_train.info())# print(y_train.info())## EDA -- object형 데이터 스케일링# print(X_train[&#x27;주구매상품&#x27;].value_counts()) ## 라벨 인코딩# print(X_train[&#x27;주구매지점&#x27;].value_counts()) ## 라벨 인코딩from sklearn.preprocessing import LabelEncoderX_train[&#x27;주구매상품&#x27;] = LabelEncoder().fit_transform(X_train[&#x27;주구매상품&#x27;])X_train[&#x27;주구매지점&#x27;] = LabelEncoder().fit_transform(X_train[&#x27;주구매지점&#x27;])X_test[&#x27;주구매상품&#x27;] = LabelEncoder().fit_transform(X_test[&#x27;주구매상품&#x27;])X_test[&#x27;주구매지점&#x27;] = LabelEncoder().fit_transform(X_test[&#x27;주구매지점&#x27;])# print(X_test.describe())# print(X_train.describe())## 총구매액, 최대구매액, 환불금액, 내점일수, 구매주기 =&gt; MinMax scalerfrom sklearn.preprocessing import MinMaxScalerX_test[[&#x27;총구매액&#x27;, &#x27;최대구매액&#x27;, &#x27;환불금액&#x27;, &#x27;내점일수&#x27;, &#x27;구매주기&#x27;]] = MinMaxScaler().fit_transform(X_test[[&#x27;총구매액&#x27;, &#x27;최대구매액&#x27;, &#x27;환불금액&#x27;, &#x27;내점일수&#x27;, &#x27;구매주기&#x27;]])X_train[[&#x27;총구매액&#x27;, &#x27;최대구매액&#x27;, &#x27;환불금액&#x27;, &#x27;내점일수&#x27;, &#x27;구매주기&#x27;]] = MinMaxScaler().fit_transform(X_train[[&#x27;총구매액&#x27;, &#x27;최대구매액&#x27;, &#x27;환불금액&#x27;, &#x27;내점일수&#x27;, &#x27;구매주기&#x27;]])# print(X_test.describe())# print(X_train.describe())## train 데이터 셋에서 훈련, 검증 데이터 분리하기# print(2482/3500) #70.9%y_train = y_train[&#x27;gender&#x27;]from sklearn.model_selection import train_test_splitX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.3, shuffle=True, random_state = 2021)print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)## 분류 모델 사용하기 (앙상블 - 에이다, gradient boost, random_forest)from sklearn.ensemble import AdaBoostClassifierfrom sklearn.ensemble import GradientBoostingClassifierfrom sklearn.ensemble import RandomForestClassifierabc = AdaBoostClassifier(random_state = 2021).fit(X_train, y_train)gbc = GradientBoostingClassifier(random_state = 2021).fit(X_train, y_train)rfc = RandomForestClassifier(random_state = 2021).fit(X_train, y_train)## 검증 데이터 성능 확인하기# print(abc.score(X_val, y_val))# print(gbc.score(X_val, y_val))# print(rfc.score(X_val, y_val))## 예측 데이터pred_abc = abc.predict(X_val)pred_gbc = abc.predict(X_val)pred_rfc = abc.predict(X_val)from sklearn.metrics import classification_report, roc_auc_score# help(classification_report)# print(classification_report(y_val, pred_abc, digits=2))# print(classification_report(y_val, pred_gbc, digits=2))# print(classification_report(y_val, pred_rfc, digits=2))prob_abc = abc.predict_proba(X_val)[:, 1]prob_gbc = gbc.predict_proba(X_val)[:, 1]prob_rfc = rfc.predict_proba(X_val)[:, 1]# print(roc_auc_score(y_val, prob_abc)) #0.649317286036036print(roc_auc_score(y_val, prob_gbc)) #0.6513388388388388# print(roc_auc_score(y_val, prob_rfc)) #0.6418703860110111##### GradientBoosting 모델로 답안pred = gbc.predict_proba(X_test)[:, 1]# 답안 제출 참고# 아래 코드 예측변수와 수험번호를 개인별로 변경하여 활용pd.DataFrame(&#123;&#x27;cust_id&#x27;: X_test.cust_id, &#x27;gender&#x27;: pred&#125;).to_csv(&#x27;003000000.csv&#x27;, index=False, encoding=&#x27;utf-8&#x27;)","categories":[],"tags":[]},{"title":"Machine Learning project","slug":"Machine-Learning-project-0","date":"2021-12-01T14:51:49.000Z","updated":"2021-12-01T14:55:10.177Z","comments":true,"path":"2021/12/01/Machine-Learning-project-0/","link":"","permalink":"https://hesthers.github.io/archives/2021/12/01/Machine-Learning-project-0/","excerpt":"","text":"Machine Learning Project ProcessWhat I did for this project Crawling best places in Sokcho, Gosung in Gangwon, S.KR Things to do tomorrow After crawling, perhaps analyze the result data to find the best place to install EV station","categories":[],"tags":[]},{"title":"Machine Learning Project","slug":"Machine-Learning-Project","date":"2021-11-30T11:39:19.000Z","updated":"2021-11-30T11:57:50.075Z","comments":true,"path":"2021/11/30/Machine-Learning-Project/","link":"","permalink":"https://hesthers.github.io/archives/2021/11/30/Machine-Learning-Project/","excerpt":"","text":"2nd Feedback 유동량을 고려할 수 있는 다른 요인들을 찾아보기 고려할 수 있는 1차 피쳐들을 확인하기 다른 보조 지표가 있으면 그것을 활용해볼것. 보조지표로서의 가치가 있는 데이터의 확보 가능성 확인하기 어디로 설정해야 가장 fit한지.. 이를테면 머무는 시간 고려하기 동일 시간대에 몰리는 전기차 수 대비 충전소 수 값 =&gt; 임계값 수치화할 수 있는 데이터가 몇 안되므로 정성적으로 갈 수 밖에 없어보임. 교통량 대신 할 수 있는 데이터: 충전소 수 =&gt; 격자별 충전소 수 &amp; 상가 수 (기존 충전소는 배제..) 격자별 필요한 충전소 수를 예측. 예측값을 뽑고 기존 충전소 수를 비교(기존 개수가 포화일 때 더이상 추가 설치가 필요가 없기 때문) 평점이 비슷한 경우나 차이가 큰 경우도 있기 때문에 스케일링이 필요함. 평점으로 유동량 예측해야 하므로 사람이 몰릴 수 있는 지역 쪽에 가중치를 좀 더 부여 사람들이 몰리는 곳이나 오래된 지점들에는 가중치를 더 많이 부여 0점을 상점으로 주는 경우 마이너스값을 가중치로 부여 (리뷰 평점으로 줄 때) 관광 명소 데이터를 요인으로 고려해볼 수 있고 충전을 많이 할 수 있는 곳은 명소나 숙박 업소 주변(머물수 있는 시간 기준으로 판단가능함.)","categories":[],"tags":[]},{"title":"project process day7","slug":"project-process-day7","date":"2021-11-29T12:23:41.000Z","updated":"2021-11-29T12:38:02.091Z","comments":true,"path":"2021/11/29/project-process-day7/","link":"","permalink":"https://hesthers.github.io/archives/2021/11/29/project-process-day7/","excerpt":"","text":"The Process of Machine Learning Project Day 7 Finish researching data Begin executing python codes by the unit of cells Have to find why the errors occur referred to my codes Difficulties It is difficult to represent the codes precisely on the map. In the geometry column, it is difficult to separate both codes respectively when the coordinates are multipolygon or polygon.","categories":[],"tags":[]},{"title":"BigData Analysis Certificate","slug":"BigData-Analysis-Certificate","date":"2021-11-28T12:55:58.000Z","updated":"2021-11-28T13:05:54.275Z","comments":true,"path":"2021/11/28/BigData-Analysis-Certificate/","link":"","permalink":"https://hesthers.github.io/archives/2021/11/28/BigData-Analysis-Certificate/","excerpt":"","text":"빅데이터 분석기사 실기시험을 준비하면서 풀어본 연습문제들 괜찮아서 공유해볼까 싶다. 문제 저작권 문제가 있을 수 있으므로 여기서는 문제보다는 링크를 공유하려 한다. kaggle bigdata 실기 연습 문제들 작업형 1유형 문제들은 나름 고민하면서 풀어볼만한 막상 시험에 나오면 코드를 어떻게 써야하지 싶은 그런 문제들이 나름 존재했다. 여기 문제들만 충분히 풀어볼 수 있다면 실기에서는 무난하게 풀 수 있지 않을까 싶다. 꽤 나름 고퀄의 문제들이다.","categories":[],"tags":[]},{"title":"What I have to study","slug":"What-I-have-to-study","date":"2021-11-27T07:52:27.000Z","updated":"2021-11-27T08:08:00.204Z","comments":true,"path":"2021/11/27/What-I-have-to-study/","link":"","permalink":"https://hesthers.github.io/archives/2021/11/27/What-I-have-to-study/","excerpt":"","text":"빅분기 단답형 공부할(복습) 개념들 정리 목록 (개인적인 용도의 기록용) 실루엣 지표 (군집분석) 의사결정나무 가지치기 &amp; 정지규칙 인공신경망 오차역전파 (역전파), 손실함수 정의 지지도, 신뢰도, 향상도 개념(설명을 보고 무엇을 나타내는지) 및 계산 혼동행렬 계산 텍스트 마이닝 전처리 과정 이상값 검출 (방법들) 데이터 베이스 (하둡 에코시스템 포함, 에코시스템에 있는 프로그램들) 시계열 성질(특징) 및 모형(AR, MA, ARIMA) 개인정보 관련 부분들 인공신경망 관련 활성화 함수들 교차검증 기법들 변수 거리 (연속형: 유클리드, 맨하탄, 민코프스키&amp; 범주형: 마할라노비스) 지니계수 계산","categories":[],"tags":[{"name":"Bigdata certificate","slug":"Bigdata-certificate","permalink":"https://hesthers.github.io/archives/tags/Bigdata-certificate/"}]},{"title":"machine learning project day4","slug":"machine-learning-project-day4","date":"2021-11-25T13:01:13.000Z","updated":"2021-11-25T13:28:08.686Z","comments":true,"path":"2021/11/25/machine-learning-project-day4/","link":"","permalink":"https://hesthers.github.io/archives/2021/11/25/machine-learning-project-day4/","excerpt":"","text":"중간 Feedback 주제 특성상 포트폴리오에서 왜 속초, 고성군을 선택했는가 설득력이 떨어지면 포트폴리오의 가치가 떨어질 수 있음 주장에 대한 근거를 데이터 수치, 자료로 보여주는 것이 훨씬 설득력이 있음. 정량적인 자료를 통해 논리적 결함이 없어야함. 데이터 역량 — 마케팅 분석, 디지털 마케팅 등을 통한 인사이트를 얻을 수 있음.. 이쪽 도메인에서 경쟁우위를 가지고 있다는 경험을 보여주기 위한 것. data 공모전 수상작 데이터 참고해서 새로운 원본 데이터 다운받아서 전처리","categories":[],"tags":[]},{"title":"Machine Learning Project","slug":"Machine-Learning-Project-2","date":"2021-11-24T11:50:05.000Z","updated":"2021-11-24T12:23:54.017Z","comments":true,"path":"2021/11/24/Machine-Learning-Project-2/","link":"","permalink":"https://hesthers.github.io/archives/2021/11/24/Machine-Learning-Project-2/","excerpt":"","text":"Machine Learning Project data preprocessing 속초시, 고성군 교통량 데이터 수집 (고속도로 관련 데이터) Difficulties 생각보다 데이터를 수집하기 어려웠고 원하는 내용이 없었음. (There are not much enough data to collect as I expected.) data is the raw data as it is, so it is difficult to combine another data.","categories":[],"tags":[]},{"title":"machine learning project day1","slug":"machine-learning-project-day1","date":"2021-11-22T13:30:02.000Z","updated":"2021-11-22T13:36:27.227Z","comments":true,"path":"2021/11/22/machine-learning-project-day1/","link":"","permalink":"https://hesthers.github.io/archives/2021/11/22/machine-learning-project-day1/","excerpt":"","text":"Machine Learning Project day 1 Topic: EV (Electronic Vehicle) charging station 여러 주제들을 생각했으나 자세한 사항들은 수요일(11/23) 회의 때 논의 내가 생각해낸 아이디어들.. (수상작 레퍼런스 참고해서) 지역별 충전소의 개수와 위치 충전소와 상관성이 있는 요인들 파악 — 상관성 파악 전기차 사업 현황 추이 (인프라 구축) 인구, 도로, 미세먼지?????, 주변환경 시설(시민 편의 시설), 교통량 전기차가 필요한 목적 (왜 필요한가..) 다른 교통수단을 이용할 수 있지 않나??? 친환경 버스와 같은…??? — 통근(유동인구) 완속/급속 충전소 — 필요한 목적 머신러닝 기법: KNN(clustering), XGBoost (편집됨) 그린뉴딜 정책, 현대 자동차 멀티 충전 전기차 (초급속 충전 가능) 온실가스 배출량 데이터","categories":[],"tags":[{"name":"project","slug":"project","permalink":"https://hesthers.github.io/archives/tags/project/"}]},{"title":"TIS","slug":"TIS","date":"2021-11-21T13:17:12.000Z","updated":"2021-11-21T13:19:17.508Z","comments":true,"path":"2021/11/21/TIS/","link":"","permalink":"https://hesthers.github.io/archives/2021/11/21/TIS/","excerpt":"","text":"Today What I StudiedThis post is for TIS page. Watched two online lecture about machine learning Practiced machine learning Finished crawling assignment on Naver news webpage","categories":[],"tags":[]},{"title":"Text Analysis","slug":"Text-Analysis","date":"2021-11-20T06:21:54.000Z","updated":"2021-11-20T07:16:37.886Z","comments":true,"path":"2021/11/20/Text-Analysis/","link":"","permalink":"https://hesthers.github.io/archives/2021/11/20/Text-Analysis/","excerpt":"","text":"Text Analysis (텍스트 분석) Sentimental Analysis Analyzing Sentiment: 긍/부정 여부 혹은 글 속에 있는 감정들 (Positive/Negative or Feelings that are in documents) Data for Review (about movies, dramas, … any digital contents) use RNN method Time series is important in sentimental analysis. 시간적/순서적인 정보가 담겨있음.Data is trained through hidden layter in serial order, then when this data has the output, the next data with the output begin training. This sequence keeps repetition.Sequence = X-featureswhen preprocessing the words, the unused words(unnecessary words) are first removed, then encoded by onehotencoding method and vectorized about the relation between words. use LSTM method pad_sequence: in the Simple RNN method case, the hidden layers are needed a lot, so the relation between words in a document are gradually lower correlated. The modeling performance would be underestimated, so the vacant space (because all of the words in sentences do not have the same length) would be padded with any number(generally 0) to cover this flaws. RNN &amp; LSTM = non-linear model","categories":[],"tags":[]},{"title":"regular expression in python","slug":"regular-expression-in-python","date":"2021-11-19T09:11:46.000Z","updated":"2021-11-19T13:50:11.325Z","comments":true,"path":"2021/11/19/regular-expression-in-python/","link":"","permalink":"https://hesthers.github.io/archives/2021/11/19/regular-expression-in-python/","excerpt":"","text":"Regular Expression 특정한 문서 안에서 표현을 찾고자 할 때 정규 표현식을 사용. 123import rere.findall(정규표현식으로 찾을 대상 (object that you want to find as the regular expression), document or variable name)re.sub(regular expression, the letters that you would replace, document or variable name) 어떤 문자 하나 찾기: x. (마침표 등의 점이 아닌 정규표현식에서 문자 한 개를 의미) 특정 문자(x) 1개 미만 (예. abacbabacb 중에서 c가 등장한 횟수가 1회 미만인 경우): x? 특정 문자(x) 1회 이상 등장한 경우: x+ 특정 문자(x) 0회 이상 등장한 경우: x* 원하는 것들중 하나를 선택: [원하는 문자들] (단, 나열할 때 구분자는 생략) 원하지 않는 것들을 제외한 나머지 중에서 하나: [^원하지 않는 문자들] 하나의 묶음을 표시할 때 혹은 묶음 안에 있는 것만을 나타낼 때: (대상 문자) 회피 용법:1) 찾고자 하는 대상이 특수 기호인 경우: 해당 글 안에서 ‘?, +’등의 문자를 찾고 싶은 경우 – re.findall(&#39;\\(특수문자)&#39;, docs)2) space, tab, enter를 찾고 싶은 경우: \\s, \\t, \\n 어떤 문자열을 찾고 싶은 경우 (길이 제한이 없음) 최대한 길게 찾고 싶을 때: .+ 최대한 짧게 여러 번 찾고 싶을 때: .+?","categories":[],"tags":[]},{"title":"machine learning basics pt.7","slug":"machine-learning-basics-pt-7","date":"2021-11-18T08:10:22.000Z","updated":"2021-11-18T08:54:00.256Z","comments":true,"path":"2021/11/18/machine-learning-basics-pt-7/","link":"","permalink":"https://hesthers.github.io/archives/2021/11/18/machine-learning-basics-pt-7/","excerpt":"","text":"MACHINE LEARNING KNN (K-Nearest Neighbors) 좌표 공간을 기준으로 해당 데이터에서 가장 가까운 데이터의 라벨을 확인하고 해당 라벨로 예측하는 기법K 값에 따라서 학습 결과가 달라짐. Clustering unsupervised learning 분류 모델인 classification과 비슷해보이나 비교하는 것은 무의미함. 비지도학습 자체로 모델링 기법으로 바라봐야함. x값이 비슷한 그룹들끼리 grouping하는 기법. X와 Y의 라벨 혹은 관계를 구분/파악하기 위한 학습방식이 아니라 비슷한 X끼리 라벨을 분류하기 위한 기법 비슷한 군집들끼리 분석해서 학습시키는 기법으로 inertia value를 기준으로 k값 선별(k의 값 = 변곡점) 1. K-Means - 각 군집의 중심값 (혹은 초깃값)을 랜덤으로 설정 및 계산하고 모델 학습 시작. - 임의의 k개 초깃값 선별해서 학습하는 기법. - Expectation: 각각의 군집에서 가장 가까운 군집과 중심을 찾아서 군집화로 분류하는 과정 - Maximization: 중심값을 업데이트 하는 과정 summary: k개의 임의의 중심값을 업데이트하는 과정을 반복하면서 최적의 군집 중심값을 찾아가는 기법 2. K-Means ++ - 임의로 아닌 처음부터 군집간의 거리가 가장 많이 떨어져있는 상태에서 초깃값을 찾는 학습과정을 선택하는 기법 - 초기에 군집을 하나만 랜덤으로 선택 - 첫번째 초깃값과 가장 먼 거리의 데이터를 중심으로 이 때의 거리를 확률값으로 판단해서 학습. Unsupervised transformation model Dimensional Reduction problem: PCA (Principle Component Analysis) 차원 축소가 필요한 이유: 차원의 저주 문제 데이터 학습에 도움되지 않은 정보들이 너무 많으면 오히려 학습에 방해가 됨. x 피처 데이터들의 분포가 전체적으로 잘 퍼져있어서 보여줄 수 있는 경우 데이터의 특성을 잘 설명해준다고 해석할 수 있음. 주성분들은 제 1주성분을 제외한 나머지 주성분들은 제1주성분과 수직으로 관계가 존재함. 수직의 관계는 서로 상관관계가 없다는 것을 의미함. (수직인 직선을 그래프를 통해 찾을 때 기울기 곱이 -1임을 생각해본다면 도움되지 않을까??) PC: 기존의 피쳐들로 성분을 select하는 것이 아닌 새로운 성분을 만들어 내서 차원을 축소하는 기법. PCA code 중 n_components의 값으로 주성분의 개수를 선택해서 모델링 1st component 가 가장 우선적인 성분으로 모델에서 가장 영향력이 큰 성분이라 할 수 있기 때문에 성분들 중 제거해야하는 경우라면 첫번째 주성분을 살려야함. 전처리할 때 자주 쓰이는 기법으로 비지도학습은 주로 지도학습 모델링을 support할 때 사용함. PCA는 여러 피처들의 조합을 통해 차원을 줄였기 때문에 학습결과가 좋게 나오는 편임. Extra info about MACHINE LEARNING Grid Search 파라미터 후보들을 사용자가 지정해주면 알아서 최적의 파라미터 값을 찾아주는 방법딕셔너리 형태로 입력해서 학습","categories":[],"tags":[]},{"title":"Design Thinking with Figma","slug":"Design-Thinking-with-Figma","date":"2021-11-17T13:26:25.000Z","updated":"2021-11-17T13:30:50.933Z","comments":true,"path":"2021/11/17/Design-Thinking-with-Figma/","link":"","permalink":"https://hesthers.github.io/archives/2021/11/17/Design-Thinking-with-Figma/","excerpt":"","text":"Figma with Design Thinking 마스터 &amp; 인스턴스 =&gt; programming화 되어있음 레이어: 페이지별 이름 설정 (변수) 프레임과 도형은 다른 것. -&gt; 어떤 것을 선택하냐에 따른 물리적 역할이 달라짐. 도형을 드래그해서 프레임 안에 넣을 수 있음. 도형 안의 도형은 만들 수 없으므로 프레임 안에 프레임으로 종속 (기능적 UI) frame selection으로 한 번에 프레임에 넣을 수 있음 프레임 속에 텍스트가 기준이 되어 입력됨. 항목 추가 +, 빼기 - 피그마는 자신만의 앱스토어를 가지고 있어서 원하는 기능을 다운받아 이용할 수 있다. 동시 협업 또한 가능하며 일반 포토샵과 같은 그래픽 툴에서는 이용하기 힘들었던 실용적이고 유용한 기능들을 이용할 수 있다. 마스터에서 인스턴스를 찍어내면 마스터에서 수정했을 때 동시에 인스턴스에서도 수정이 가능함. 마름모 모양의 메뉴 = 인스턴스 인스턴스 부분 클릭하고 design에서 ...에서 detach instance 클릭하면 인스턴스 해제 combine as variant =&gt; 그룹화 ctrl + D 누르면 복제 여러 개 가능함. json 키 값에 맞추어서 json 파일을 ui 부분에 넣을 수 있음. 프로토타이핑 기능: 화면 전환설정 Why Figma is useful in Design Thinking? 보이지 않는 문제점들을 찾아내는 것이 중요하며 정성적인 관찰이 상당히 중요함. 공감을 통한 아이디어 전개 정성적으로 만들어서 실용적으로 이용할 수 있음. 피그마를 통해 구현 가능함.","categories":[],"tags":[]},{"title":"machine learning pt.6","slug":"machine-learning-pt-6","date":"2021-11-16T08:12:41.000Z","updated":"2021-11-16T13:23:14.458Z","comments":true,"path":"2021/11/16/machine-learning-pt-6/","link":"","permalink":"https://hesthers.github.io/archives/2021/11/16/machine-learning-pt-6/","excerpt":"","text":"Machine Learning pt.6 Decision Tree non-linear model Through the trained model, this model predicts the result values. Using step-by-step questions, the groups are divided by the similar groups hat have the similar size. Information Gain: How many information you can get as you also ask many questions? Impurity metrics: Entropy &amp; Gini coefficient (purity = 100% means impurity = 0, so stop the modeling)Following that the cost is lowering, find the questions that lower the entropy, but specify the number of features.According to the depth, the complexity of a model is decided. recursive partitioning = repeatedly split the data because overfitting problem can occur summary: Decision Tree is simple when the depth of question is low, vice versa. However, the degree of training for the training data is high but for the test data is not. Decision Tree is explainable that the reason is obvious about the result of prediction (how it classifies the data using which criteria) Decision Tree can be used as the Regressional model. Variance is the error ofdecision tree regressor. the value of entropy is large when the labels are even. Ensemble Bagging(Bootstrap Aggregating) Bootstrap means extracting the copy data from the original data.Aggregating means extracting the results from the different models with the majority vote.example: The results from three different models are the following: A: X with 0.4 probability, B: O with 0.9 probability, C: X with 0.3 probability Average probability is about 0.5. The result ratio is 2:1 = X:O. Q: Which result of models is correct? Like above an example, you can choose soft or hard voting method in Bagging.When extracting the copy data, extract it with sampling with replacement (random sampling). Random Forest: combining various decision tree models with reinforcing the generalization by training the models independently and voting the one of the results, but underfitting problem often occurs. Boosting Adaboost / GBM / XGBoost / Light GBM the way to train is the same as Bagging, but the process of training is different.one model is trained first and then after the results of modeling are identified, imposing the weights on the wrong prediction and start training the next model (repeating these process) As influenced by the previous model training, the models are not independent.The models of boosting are different from the performance. The performance of Light GBM is the fastest model in the training.","categories":[],"tags":[]},{"title":"machine learning basics pt.5","slug":"machine-learning-basics-pt-5","date":"2021-11-15T08:26:11.000Z","updated":"2021-11-15T15:17:43.032Z","comments":true,"path":"2021/11/15/machine-learning-basics-pt-5/","link":"","permalink":"https://hesthers.github.io/archives/2021/11/15/machine-learning-basics-pt-5/","excerpt":"","text":"Machine Learning BasicsLinear Classification Model Metrics Different from linear regression, Logistics regression and SVM needs the metrics to test their performance. the metrics of Linear Regression = $\\displaystyle R^2$ score Be careful! Do not say $\\displaystyle R^2$ score as accuracy score. the metrics of Classification model = when the label is imbalanced the case when the accuracy score is high but is not able to ensure the performanceExample: The medical test kit of the accuracy is 99%. Do you trust this score? IF the data is imbalanced, it is difficult to predict accurately the data. (Lower modeling performance) Confusion Matrix (Real) Positive (Real) Negative (Prediction) Positive TP FP (Prediction) Negative FN TN Precision(정밀도, evaluation as prediction = positive) = $\\displaystyle \\frac{TP}{(TP + FP)}$Recall(재현율, evaluation as real = positive) = $\\displaystyle \\frac{TP}{(TP + FN)}$ Regulation of Classification model the case when regulation is necessary: when the number of weight is a lot in order to restrain overfitting problem (the model is complicated = reinforcement of regulation = lowering C) cost function formula of classification model =$\\displaystyle error + \\frac{1}{C} \\times \\vert w \\vert = \\\\\\\\C(error + \\frac{1}{C} \\times \\vert w \\vert) = \\\\\\\\C \\times error + \\vert w \\vert$ C = the rate of error tolerance SVM &amp; SVC SVC (Support Vector Classifier) linear &amp; non-linear as parameter set parameter through kernel to impose the non-linearity margin = $\\displaystyle \\frac{1}{C}$ To maximize the margin, allow the error by lessening the value of C to lower overfitting problem and the error Hard SVM (perform the modeling without allowing the error at all) vs Soft SVM (select the optimized margin with allowing the error and selecting the data that is in the margin) prediction value error range of error Logistic Regression $\\displaystyle \\frac{1}{(1 + e^{-wx})}$ (the value of probability) log loss $\\displaystyle 0 &lt; n &lt; \\infty$ SVM $\\displaystyle \\begin{cases} 1 \\\\ 0 \\end{cases}$ hinge loss $\\displaystyle 0 \\leq n &lt; \\infty$ Kernel: change non-linear coordinates into linear coordinates$\\displaystyle \\implies$ MLP classifier (using rbf formula and various ways, find the linear relation) $\\displaystyle \\rightarrow$ Deep learning and NN(Neural Network) Perceptron: 비선형성이 굉장히 복잡한 경우 층을 쌓아가면서 비선형성 문제를 풀릴수 있도록 해줌.","categories":[],"tags":[]},{"title":"machine learning basics pt.4","slug":"machine-learning-basics-pt-4","date":"2021-11-13T11:59:41.000Z","updated":"2021-11-13T14:24:54.485Z","comments":true,"path":"2021/11/13/machine-learning-basics-pt-4/","link":"","permalink":"https://hesthers.github.io/archives/2021/11/13/machine-learning-basics-pt-4/","excerpt":"","text":"Machine Learning basics pt.4comparison of Lasso and Ridge Lasso Ridge 모델링을 하다보면 규제항0이 되는 값들이 존재함 아무리 모델링을 해도 규제항이 0에 가까워지지 않음 $\\displaystyle 기울기(G) = 오차’ + C$ $\\displaystyle 기울기(G) = ( 오차’ + \\vert w \\vert )$ $\\displaystyle W_{t} - a(오차’ + C)$ $\\displaystyle W_{t} - a( 오차’ + \\vert w \\vert )$ 피처가 많은 경우 (= weight가 많은 경우)에 강하게 규제를 걸어야할 때 피처선택을 위해 사용 Linear Classfication Model classification: 확률/비확률적 모형 &amp; 선형/비선형적 모형 확률적으로 분류한다 = 분류 경계선을 기준으로 데이터 확률값으로 예측한다. Logistic (= 확률적 예측한다) Regression (= 선형적 모형) 예측값 = 확률값 0과 1로 구성된 데이터 값들을 가지고 집단을 구분짓는 경계선(=&gt; wx = 0)을 찾아 특정 카테고리 값을 예측하는 기법. hypothesis: $\\displaystyle \\begin{equation} \\frac{1}{(1+e^{-wx})}\\end{equation}$ cost: 절댓값의 오차는 로지스틱 회귀분석에서 의미가 없다. (아무리 최적화를 위해 cost가 낮아지는 방향으로 가중치를 업데이트해도 cost값이 낮기 때문에 학습이 잘 되지 않기 때문.) 그래서 로그함수를 cost 함수로 이용함. Local Minimum problem local minimum이 많은 데이터의 경우 학습이 잘 안되어 underfit이 발생할 확률이 높다. 그래서 복잡한 모델인 경우 underfit에 취약한 경우이므로vanishing gradient(기울기 소실) 문제를 만날 수 있다. (cost값을 미분해서 weight값을 업데이트 해나가기 때문임.) Adam optimizer와 같은 모델들을 이를 해결하기 위한 방법으로 발전했다. 다중 분류의 경우는 소프트맥스 함수를 이용해 해결하고 경계선을 기준으로 확률적으로 예측한 값들을 각각 계산한 뒤 최종적으로 큰 확률값을 갖는 카테고리의 값으로 예측함. SVM (Support Vector Machine) Linear SVM의 경우, 비확률적 선형 모형 로지스틱의 선형 경계선 (wx = 0)은 그대로 이용하면서 hypothesis의 함수를 다른 즉, 비확률적 선형 모형으로 구해야함. hypothesis: $\\begin{cases} 1 &amp; \\quad (x \\geq 0)\\\\\\\\ 0 &amp; \\quad (x &lt; 0)\\end{cases}$ 데이터가 경계선과 인접할수록 overfitting의 우려가 있기 때문에 데이터와의 거리를 최대한 확보하면서 경계선을 그리는 것이 좋다. 마진 최대화하며 학습 cost = C (오차허용률) x 오차 + 마진의 역수항 (1 / 마진 = 규제항 x (1/2)) 라쏘 규제항을 이용하는 경우: $\\displaystyle \\begin{equation} 오차 + \\cfrac{1}{\\cfrac{2}{\\sum |w|}} \\end{equation} \\\\\\\\ = 오차 + \\frac{1}{2} \\times \\sum |w|$ 오차허용률: 허용률 값이 크면 오차 허용을 안하기 때문에 오차값은 작아지고 마진의 역수값이 커지게 되는데 마진은 작은 값을 갖게 되기 때문에 overfitting이 발생할 수 있다. 그래서 오차가 커지더라도 오차허용률 값을 줄여서 오차허용을 어느 정도 해주는 것이 좋음.","categories":[],"tags":[]},{"title":"machine learning pt.3","slug":"machine-learning-pt-3","date":"2021-11-12T12:23:04.000Z","updated":"2021-11-12T13:12:18.037Z","comments":true,"path":"2021/11/12/machine-learning-pt-3/","link":"","permalink":"https://hesthers.github.io/archives/2021/11/12/machine-learning-pt-3/","excerpt":"","text":"Machine learning pt.3The reason why the regulation in linear regression is necessary when the weights are too many in linear regression, the sizes of the weights are also bigger. so, the model would be complicated, and then the possibility of overfitting would occur. To restrain this modeling complexity, the regulation is needed. The regulationPlease refer to a table on the previous post about comparison of ridge, lasso, elastic net. However, too much regulation can lead to the underfitting of the data. Lasso (L1 norm) regulation: if a feature would be not 0 value even though the user imposes the lasso to give a penalty, the feature must be a significant feature. (a role of feature selection to eliminate many of features) formula for Elastic Net‘s cost function:$\\displaystyle weight(error) + \\lambda 1 \\times \\sum |w| + \\lambda 2 \\times \\sum w^2 \\\\\\\\= weight(error) + \\lambda(l_{l ratio} \\sum |w| + (1 - l_{l ratio}) \\sum w^2)$ Multi-linear regression A kind of linear regression that has some features, not only one feature. Through the value of weight, it is possible to identify with the influence of each of X features. types of Linear Classification model Logistic regression SVM (Support Vector Machine) SPOILER of the next post!!It would be continued regarding about Linear Classification models. It would also be about python codes for machine learning with some explanation!","categories":[],"tags":[]},{"title":"machine learning pt.2","slug":"machine-learning-pt-2","date":"2021-11-11T12:01:19.000Z","updated":"2021-11-12T13:01:21.208Z","comments":true,"path":"2021/11/11/machine-learning-pt-2/","link":"","permalink":"https://hesthers.github.io/archives/2021/11/11/machine-learning-pt-2/","excerpt":"","text":"Regression model and regulationThis post is about linear regression and regulation part. 전체적인 경향을 판단할 수 있는 지표 혹은 수치는 weight의 크기라 할 수 있다. (the index to identify overall tendency is the size of weight because the overfitting occurs if the data of training test is lessened.) using the value of weight, repeat the training of the machine. the regulation to restrain the modeling complexity Summary of this post Linear Ridge Lasso Elastic Net hypo &lt;—- wx + b —- —-&gt; cost 오차항 오차항 + L2 Norm 오차항 + L1 Norm 오차항 + L1 Norm + L2 Norm L1 norm and L2 norm regulation — for model handlingL1 Norm = $\\displaystyle \\sum |w|$L2 Norm = $\\displaystyle \\sum w^2$ Linear Regression relatively simple model the hypothesis of linear regression (for the correlation between X and Y) =&gt; wx + b 미지수가 많은 경우, 해가 무수히 많다.라는 말은 새로운 데이터가 입력되었을 때 무엇인지 정확하게 예측하기 어렵다와 같은 말. 학습데이터는 잘 예측하지만 새로운 데이터가 추가되면 예측도가 떨어지는 것이므로 overfitting이 발생하기 쉽다.when the number of weight in linear regression, the regulation is necessary.the more the number of weight is added, the more complex the model.","categories":[],"tags":[]},{"title":"machine learning basics pt.1","slug":"machine-learning-basics-pt-1","date":"2021-11-10T11:31:46.000Z","updated":"2021-11-11T13:16:17.922Z","comments":true,"path":"2021/11/10/machine-learning-basics-pt-1/","link":"","permalink":"https://hesthers.github.io/archives/2021/11/10/machine-learning-basics-pt-1/","excerpt":"","text":"Machine Learning basicsWhat is Machine Learning? a part of AI field (Artificial Intelligence) Training-based technology: as using data and training data, the machine itself can make own algorithm by finding the rules or patterns. without being explicitly programmed purpose of machine learning finding the good (or optimized hypothesis) that the cost (or error) is low but, before entering the data, the user has to first design the specific hypothesis when the user does not know about the formula. to avoid the overfitting, regulation for the machine learning (by lessening the size of the weight) is needed.argmin =$\\begin{matrix}min &amp; cost \\\\weight(= \\theta) &amp;\\end{matrix}$ How to train the machine? calculate the value of cost (with the prediction using the first hypothesis set by the user) and then, differentiate the value of cost to find the slope. the initial weight is for lowering the value of cost W(t+1) &lt;= Wt - a * Gradient (Repeating this process) The dataset for machine learning a part of dataset is for training, the rest of the dataset is for validation. when the result of the training data is good but the result of validation is not, the training is useless.","categories":[],"tags":[]},{"title":"Today What I Studied","slug":"Today-What-I-Studied-0","date":"2021-11-09T12:38:32.000Z","updated":"2021-11-09T12:42:41.249Z","comments":true,"path":"2021/11/09/Today-What-I-Studied-0/","link":"","permalink":"https://hesthers.github.io/archives/2021/11/09/Today-What-I-Studied-0/","excerpt":"","text":"TIS Watched online lecture about machine learning how to treat the missing valuevariable distribution problemclass imbalancecurse of dimensionality Will practice the machine learning for big data certificate","categories":[],"tags":[]},{"title":"result of preprocessing data","slug":"result-of-preprocessing-data","date":"2021-11-08T14:23:16.000Z","updated":"2021-11-08T14:41:10.023Z","comments":true,"path":"2021/11/08/result-of-preprocessing-data/","link":"","permalink":"https://hesthers.github.io/archives/2021/11/08/result-of-preprocessing-data/","excerpt":"","text":"전처리 결과movie_train 데이터의 전처리 결과를 분석했다. (I summarized the result of preprocessing movie_train data here.) 전처리 이전 num_actor라는 변수는 상관도가 낮은 변수이기 때문에 전처리 이전에 삭제 후 전처리를 진행한다. (I will continue preprocessing after removing num_actor column because this column is less correlated.) 범주형 데이터를 가진 컬럼들은 인코딩을 해주고 수치형 데이터로 변환되고 난 후 상관관계를 확인했을 때 날짜관련 데이터 부분들은 삭제해준다. (After encoding the columns that has nominal data as label and with onehot encoding and converted into numeric data, the columns related to date are removed.) &#39;scaled_director&#39;, &#39;scaled_genre&#39;, &#39;scaled_distributor&#39; columns are also removed because of the low correlation. 최종적으로 이용해도 될만한 컬럼들은 box_off_num, dir_prve_bfnum, dir_prev_num, num_staff, screening_rat (onehot encoding columns), time이다. (물론 회귀분석에서 다중공선성 여부로 전체 컬럼들을 가지고 다시 확인할 생각이다.)(The columns that are able to utilize are ultimately box_off_num, dir_prve_bfnum, dir_prev_num, num_staff, screening_rat (onehot encoding columns), time. (Of course, I also recheck the total columns with multicollinearity later in Linear regression modeling.)","categories":[],"tags":[]},{"title":"practice coding website","slug":"practice-coding-website","date":"2021-11-07T13:35:58.000Z","updated":"2021-11-07T13:39:51.310Z","comments":true,"path":"2021/11/07/practice-coding-website/","link":"","permalink":"https://hesthers.github.io/archives/2021/11/07/practice-coding-website/","excerpt":"","text":"I recommend this website to practice coding with various language.만약 코딩 연습하고 싶다면 아래의 웹사이트에서 해보는 것을 추천한다. 물론 일반 주피터노트북, 아니콘다 등 실행환경과는 약간 다르기 때문에 처음 사용하면 불편함을 느낄 수 있으나 익숙해지면 괜찮은 것 같다. Groom IDE This website is for korean, but you can change the different language, English and Japanese.","categories":[],"tags":[]},{"title":"EDA 2","slug":"EDA-2","date":"2021-11-06T13:45:15.000Z","updated":"2021-11-06T14:04:25.805Z","comments":true,"path":"2021/11/06/EDA-2/","link":"","permalink":"https://hesthers.github.io/archives/2021/11/06/EDA-2/","excerpt":"","text":"EDA pt.21234567891011121314151617181920df = pd.pivot_table(data = movie_train, columns=&#x27;screening_rat&#x27;, index = &#x27;genre&#x27;, values= [&#x27;director&#x27;], aggfunc=len, )df_rate = [i[1] for i in df.columns]reset_df = pd.DataFrame(df.values, columns = sorted(movie_train.screening_rat.unique()))reset_df.fillna(0, inplace=True)reset_df[&#x27;index&#x27;] = sorted(movie_train.genre.unique())reset_df = reset_df.set_index(reset_df[&#x27;index&#x27;], drop=True)reset_df.drop(columns = &#x27;index&#x27;, inplace=True, axis=1)from matplotlib import font_manager, rcrc(&#x27;font&#x27;, family = &#x27;Malgun Gothic&#x27;)plt.figure(figsize = (20 , 16))sns.histplot(data = reset_df.T, multiple=&#x27;dodge&#x27;, bins = 3)plt.xlim([0, 65])plt.show()plt.rcParams[&#x27;axes.unicode_minus&#x27;] = Falseplt.figure(figsize = (12 , 8))sns.kdeplot(data = reset_df.T, multiple=&#x27;layer&#x27;)plt.show()","categories":[],"tags":[]},{"title":"Design Thinking process","slug":"Design-Thinking-process","date":"2021-11-05T09:18:03.000Z","updated":"2021-11-05T09:26:28.637Z","comments":true,"path":"2021/11/05/Design-Thinking-process/","link":"","permalink":"https://hesthers.github.io/archives/2021/11/05/Design-Thinking-process/","excerpt":"","text":"Design Thinking Process 사건을 디자인하라. (Design the event/context) 고급 호텔의 레스토랑의 식탁보 = 당신만을 위한 것 데이터로 추출되지 않는 인사이트를 도출해내는 것. 대상과 상황을 분리하지 말아라. (Do not let objects and context separate) 대상이 존재하는 컨텍스트 (상황)과 분리하여 볼 수 없음. “상대방을 이해하기 위해 상황을 이해하는 것.” 예. 병원과 MRI의 상황을 분리할 수 없음. 상황을 잘 살펴보는 것. 대상이 맥락에서 어떤 존재인가 주관적 관점과 객관적 관점으로 바라보라. (Look somthing subjectively and objectively) 관찰하고 관찰하라 (Look at everything again and again) 파괴적 혁신 아이디어를 이용해라. (Hitting on a good idea, write it all)","categories":[],"tags":[]},{"title":"Design Thinking","slug":"Design-Thinking","date":"2021-11-04T12:18:53.000Z","updated":"2021-11-04T12:47:19.342Z","comments":true,"path":"2021/11/04/Design-Thinking/","link":"","permalink":"https://hesthers.github.io/archives/2021/11/04/Design-Thinking/","excerpt":"","text":"Design Thinking오랜 시간 동안 디자인과 마케팅 그리고 데이터는 뗄래야 뗄 수 없는 필수불가결한 존재이다. 사실 인류가 살아오면서 디자인과 함께 존재해왔다고 해도 과언이 아니다. 고대 그리스 아니 어쩌면 그 전부터 인류가 나타나기 시작한 그 때부터가 아닐까 싶다. 인류가 살면서 인류에 최적화된 도구들을 만들고 하면서부터 디자인이 시작된 건 아닐까라는 생각을 해본다. 인류가 수없이 전쟁 등을 치루면서 생존의 문제를 겪어왔다. 그러면서 어떻게 해야 살아남을까하는 그런 생존 문제들을 해결하기 위해 어떠한 관점으로 문제를 바라왔을까 이러한 생각 또한 디자인이 아니였을까. 디자인 씽킹에서 중요하게 보는 것이 사람의 관점, 어떻게 상황과 문제를 분석하고 해결할지에 대한 것이다. 어떤 직업을 갖고 있느냐에 따라 디자인의 적용 분야와 의미가 다르다. 단순히 눈에 보이는 작품적인 면에서의 디자인은 흔하다. 눈에 보이는 것은 뻔하니까. 그러나 여기에 자신만의 가치와 쓸모있는 기능을 더하면 그것은 아주 유용한 디자인이 된다. 그러나 모든 문제들은 눈에 띄지 않는다. Invisible problem. 이걸 찾아내는 것이 의미있는 행위라 할 수 있다. 일반적으로 데이터 분석에서 인사이트를 도출한다하는 것은 데이터 자체만 가지고 바라봤을 때 눈에 보이지 않는 해석을 찾아야 한다는 것과 같은 셈이다. 우리가 일반적으로 겪는 상황들 속에서 이건 불편한 것, 이랬으면 좋겠다 싶은 것들이 꽤 있다. 그 속에서 겪는 불편함을 해소하고자 디자인을 하고 문제를 해결하기도 한다. 예를 들어보자면, 장애인을 위한 저상버스가 아닐까 생각해본다. 타인에 대한 이해. 심리학에서도 다루는 이 심리적인 부분을 이용하여 마케팅에 이용하듯 디자인에서도 이 섬세한 배려가 엿보이기도 한다. 혁신, 디자인, 가치, 욕구와 욕망. 오늘 디자인 씽킹 수업을 듣고 난 후 들었던 생각들을 한 번 정리해봤다…","categories":[],"tags":[]},{"title":"TIS (Today What I Studied)","slug":"TIS-Today-What-I-Studied","date":"2021-11-03T14:49:22.000Z","updated":"2021-11-03T14:53:07.747Z","comments":true,"path":"2021/11/03/TIS-Today-What-I-Studied/","link":"","permalink":"https://hesthers.github.io/archives/2021/11/03/TIS-Today-What-I-Studied/","excerpt":"","text":"TIS watched the online lectures about: how to preprocess the data effectivelywith EDA introduction how to read files that have the different types in python indexing &amp; masking with data structurereading filesmerge and concatmatplotlib visualization","categories":[],"tags":[]},{"title":"What I think about Big Data","slug":"What-I-think-about-Big-Data","date":"2021-11-02T13:31:59.000Z","updated":"2021-11-02T13:46:46.431Z","comments":true,"path":"2021/11/02/What-I-think-about-Big-Data/","link":"","permalink":"https://hesthers.github.io/archives/2021/11/02/What-I-think-about-Big-Data/","excerpt":"","text":"My idea오늘은 내가 빅데이터를 공부하면서 느꼈던 것들을 잠시 적어볼까 싶다. 사실 빅데이터를 공부하게 된 것은 아주 우연이였다. 원래는 금융분야를 전공으로 공부하고 있었는데 우연치않게 빅데이터를 접하게 되고 진로를 고민해보게 되었다. 마케팅 개론 수업때 빅데이터라는 용어를 접한게 다였고 사실 생각도 못했다. 그러나 취업을 포함해 내가 가장 관심을 가지고 열심히 해볼 수 있는 분야가 무엇일까 고민하던 중 접한 영역이 빅데이터였다. 미국 기업에 인턴 준비하려고 하던 중 자격요건으로 빅데이터 관련 스킬을 요구하는 기업들이 많아서 결국은 포기했었다. 코딩도 전혀 모르고 파이썬? R? 이런 건 대체 뭔지도 모르는 내가 이것들을 공부하면서 앞으로는 데이터는 절.대. 무시할 수 없는 중요한 분야임을 깨달았다. 코딩하면서 결과가 잘 안나오거나 오류가 계속 나면 노트북을 때로는 내려치고 싶을 때도 있지만 그래도 끝까지 해보면서 며칠이 걸려도 결과물이 나오면 뿌듯할 때도 많다. 부족하지만 그래도 코딩을 해나가다보면 얻는 인사이트들도 많다. 일상의 모든 것들이 빅데이터이다. 지금 이순간 내가 github 블로그에 코딩으로 포스팅하면서 올리는 포스트들 모두 데이터인 셈이다. 나중을 되돌아 봤을때 지금 공부해왔던 것들이 하나씩 쌓여가며 큰 도움이 되지 않을까 싶다. 쉽게 얻은 것들은 나중에 크게 대가를 치르게 된다. 그렇기 때문에 자격증도 지금 빅데이터를 공부하고 있는 모든 것들이 쉽게 얻어진 것들이 아니기에 큰 빛을 발하게 되리라 기대해본다.","categories":[],"tags":[]},{"title":"Today what I studied","slug":"Today-what-I-studied","date":"2021-11-01T11:59:31.000Z","updated":"2021-11-01T12:11:45.987Z","comments":true,"path":"2021/11/01/Today-what-I-studied/","link":"","permalink":"https://hesthers.github.io/archives/2021/11/01/Today-what-I-studied/","excerpt":"","text":"TIS (Today What I studied)Today is the most busy and exhausting day… I watched the online lecture video about handling data, including data preprocessing, to prepare for the new project. The lecture is about: data structure: list, tuple, dictionary for iteration Numpy Pandas: Series, Data Frame I also practiced the machine learning to prepare for Bigdata certificate.","categories":[],"tags":[]},{"title":"EDA 1st","slug":"EDA-1st","date":"2021-10-31T12:15:48.000Z","updated":"2021-10-31T12:21:39.599Z","comments":true,"path":"2021/10/31/EDA-1st/","link":"","permalink":"https://hesthers.github.io/archives/2021/10/31/EDA-1st/","excerpt":"","text":"After preprocessing the missing valuesThe next step is EDA after preprocessing the missing values in DataFrame. Figuring out the lowest correlation, then dropping the columns that are less correlated. The following codes are for EDA. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768movie_train.corr()sns.heatmap(data = movie_train.corr(), annot=True)movie_train.corr()[movie_train.corr() &gt;= 0.3]movie_train.drop(columns = &#x27;num_actor&#x27;, inplace=True)sns.heatmap(data = movie_train.corr(), annot=True)col_mt = movie_train.columns.difference([&#x27;num_staff&#x27;])movie_train[col_mt].corr()sns.pairplot(movie_train[col_mt])### Title, distributor, genre, release_time, screening_rat, directordistributor = movie_train.distributor.value_counts().sort_values(ascending=True)distributor = distributor[distributor &gt; 2]distributor_name = distributor.index.tolist()distributor_cnt = distributor.values.tolist()plt.rc(&#x27;font&#x27;, family = &quot;Malgun Gothic&quot;)fig=plt.figure(figsize = (15,10))ax = fig.add_subplot(1,1,1)sns.barplot(distributor_name, distributor_cnt, data = movie_train)plt.xlabel(&#x27;배급사&#x27;, fontsize = 15)plt.title(&#x27;영화 배급사 수&#x27;, fontsize = 20)ax.set_xticklabels(distributor_name, rotation=75)plt.show() #------------------genre = movie_train.genre.value_counts().sort_values(ascending=True)genre = genre[genre &gt; 1]genre_name = genre.index.tolist()genre_cnt = genre.values.tolist()plt.rc(&#x27;font&#x27;, family = &quot;Malgun Gothic&quot;)fig=plt.figure(figsize = (15,10))ax = fig.add_subplot(1,1,1)sns.barplot(genre_name, genre_cnt, data = movie_train)plt.xlabel(&#x27;장르&#x27;, fontsize = 15)plt.title(&#x27;장르별 영화 수&#x27;, fontsize = 20)ax.set_xticklabels(genre_name, rotation=75)plt.show() #--------------------rate = movie_train.screening_rat.value_counts().sort_values(ascending=True)rate = rate[rate &gt; 1]rate_name = rate.index.tolist()rate_cnt = rate.values.tolist()plt.rc(&#x27;font&#x27;, family = &quot;Malgun Gothic&quot;)fig=plt.figure(figsize = (15,10))ax = fig.add_subplot(1,1,1)sns.barplot(rate_name, rate_cnt, data = movie_train)plt.xlabel(&#x27;등급&#x27;, fontsize = 15)plt.title(&#x27;등급별 영화 수&#x27;, fontsize = 20)ax.set_xticklabels(rate_name, rotation=75)plt.show() #----------------------director = movie_train.director.value_counts().sort_values(ascending=True)director = director[director &gt; 1]director_nm = director.index.tolist()director_cnt = director.values.tolist()plt.rc(&#x27;font&#x27;, family = &quot;Malgun Gothic&quot;)fig=plt.figure(figsize = (15,10))ax = fig.add_subplot(1,1,1)sns.barplot(director_nm, director_cnt, data = movie_train)plt.xlabel(&#x27;감독명&#x27;, fontsize = 15)plt.title(&#x27;감독별 영화 수&#x27;, fontsize = 20)ax.set_xticklabels(director_nm, rotation=75)plt.show() I added the visualization codes about the nominal data. I am not done yet! The codes are just the first step of EDA.","categories":[],"tags":[{"name":"practice for machine learning","slug":"practice-for-machine-learning","permalink":"https://hesthers.github.io/archives/tags/practice-for-machine-learning/"}]},{"title":"practice machine learning","slug":"practice-machine-learning","date":"2021-10-30T13:34:57.000Z","updated":"2021-10-30T13:49:35.398Z","comments":true,"path":"2021/10/30/practice-machine-learning/","link":"","permalink":"https://hesthers.github.io/archives/2021/10/30/practice-machine-learning/","excerpt":"","text":"This post is for a series of practice for machine learning. I used movie data in order to practice the machine learning by preparing for the big data certificate test. The python code below is just to preprocess the data. (Today I will post only codes of preprocessed training data.) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384# import modules &amp; filesimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoderpd.set_option(&#x27;display.max_columns&#x27;, 30)movie_train = pd.read_csv(&#x27;./movies_train.csv&#x27;, encoding = &#x27;utf8&#x27;)movie_train.head(2)movie_train.info()sns.heatmap(data = movie_train.corr(), annot=True)movie_train.corr()[movie_train.corr() &gt;= 0.3]########## Figuring out missing valuemovie_train[movie_train.dir_prev_bfnum.isnull()]movie_train.dir_prev_bfnum.isnull().sum()movie_train.isnull().sum()[movie_train.isnull().sum() &lt;= len(movie_train) * 0.7]movie_train.director.value_counts()idx_dir_bfn = [n for n in movie_train[&#x27;director&#x27;].values]null_idx_dir_bfn = [movie_train[[&#x27;director&#x27;, &#x27;dir_prev_bfnum&#x27;]][movie_train[&#x27;dir_prev_bfnum&#x27;].isnull()].values[m][0] for m in range(movie_train[&#x27;dir_prev_bfnum&#x27;].isnull().sum())]nn_idx_dir_bfn = [movie_train[[&#x27;director&#x27;, &#x27;dir_prev_bfnum&#x27;]][movie_train[&#x27;dir_prev_bfnum&#x27;].notnull()].values[m][0] for m in range(movie_train[&#x27;dir_prev_bfnum&#x27;].notnull().sum())]len(movie_train[&#x27;director&#x27;][movie_train[&#x27;dir_prev_bfnum&#x27;].isnull()])len(set(idx_dir_bfn))len(movie_train[&#x27;director&#x27;][movie_train[&#x27;dir_prev_bfnum&#x27;].notnull()].unique())dir_nm = []for j in nn_idx_dir_bfn: for i in null_idx_dir_bfn: if i not in j: pass elif i in j: dir_nm.append(i)dir_nm = set(dir_nm)bf_num = [movie_train[&#x27;dir_prev_bfnum&#x27;][movie_train.director == n].values for n in dir_nm]df = pd.DataFrame(bf_num)df.drop(columns = [4, 5, 6], inplace=True, axis=1)df[&#x27;dir_nm&#x27;] = dir_nmdfdf.fillna(0, inplace=True)zero_cnt = [list(df.iloc[i, 0:4].values).count(0) for i in range(0,len(df))]tot_bfnum = []df_mean = []for i in range(0,len(df)): if list(df.iloc[i, 0:4].values).count(0) == 1: tot_bfnum = np.delete(df.iloc[i, 0:4].values, list(df.iloc[i, 0:4].values).index(0)) df_mean.append(np.mean(tot_bfnum)) elif list(df.iloc[i, 0:4].values).count(0) == 2: tot_bfnum = np.delete(df.iloc[i, 0:4].values, list(df.iloc[i, 0:4].values).index(0)) tot_bfnum = np.delete(tot_bfnum, list(tot_bfnum).index(0)) df_mean.append(np.mean(tot_bfnum)) elif list(df.iloc[i, 0:4].values).count(0) == 3: tot_bfnum = np.delete(df.iloc[i, 0:4].values, list(df.iloc[i, 0:4].values).index(0)) tot_bfnum = np.delete(tot_bfnum, list(tot_bfnum).index(0)) tot_bfnum = np.delete(tot_bfnum, list(tot_bfnum).index(0)) for i in list(tot_bfnum): df_mean.append(i)df[&#x27;mean_bfnum&#x27;] = df_meandir_bfnum_dict = &#123;nm : num for nm, num in zip(df[&#x27;dir_nm&#x27;], df[&#x27;mean_bfnum&#x27;])&#125;mt = pd.merge(movie_train, df[[&#x27;dir_nm&#x27;, &#x27;mean_bfnum&#x27;]], how = &#x27;outer&#x27;, left_on = &#x27;director&#x27;, right_on = &#x27;dir_nm&#x27;)for n in dir_nm: movie_train[&#x27;dir_prev_bfnum&#x27;][(movie_train.director == n) &amp; (movie_train[&#x27;dir_prev_bfnum&#x27;].isnull())] = movie_train[&#x27;dir_prev_bfnum&#x27;][(movie_train.director == n) &amp; (movie_train[&#x27;dir_prev_bfnum&#x27;].isnull())].apply(lambda x:dir_bfnum_dict.get(n))movie_train.info()movie_train.fillna(0, inplace=True) I filled the missing values by mean of each director’s the number of previous cinema audiences. Please do not copy codes for commercial purpose. (The copyright of data and the question owned by Dacon)","categories":[],"tags":[{"name":"practice for machine learning","slug":"practice-for-machine-learning","permalink":"https://hesthers.github.io/archives/tags/practice-for-machine-learning/"}]},{"title":"check point for data preprocessing","slug":"check-point-for-data-preprocessing","date":"2021-10-29T14:04:42.000Z","updated":"2021-10-29T14:17:40.156Z","comments":true,"path":"2021/10/29/check-point-for-data-preprocessing/","link":"","permalink":"https://hesthers.github.io/archives/2021/10/29/check-point-for-data-preprocessing/","excerpt":"","text":"Data Preprocessing오늘 어떤 내용을 올릴까 고민하다가 갑자기 문득 이걸 해보려고 한다. (소재가 없어서다…) 데이터 전처리! 데이터 전처리는 사실 데이터 분석에서 아주 중요한 부분이다. 전처리를 하지 않은 경우 수치상의 상관계수라던가 모델링에서 문제가 발생할 수 있어서다. 과적합 등의 문제가 발생하게 되면 예측 모델이나 분류 모델 등 성능이 떨어지기 때문이다. 이 전처리 부분을 다루는 이유는 어제 빅분기 실기시험 대비 머신러닝 연습하다가 깨달은 부분이기도 하다. 데이터 전처리에서 낑낑거렸는데 알고보니 잘못 방향을 잡아서 데이터 프레임 병합이 안되거나 상관계수가 너무 낮게 나오기도 했다. 그래서 데이터를 불러오고 처음에는 결측값 등을 먼저 확인해야한다. 그리고 결측값을 파악하고 나면 describe (파이썬) 으로 수치형 데이터들의 정보를 확인해봐야 한다. 어느 정도 정리가 되었다면 명목형 데이터 부분들을 확인하고 불필요한 컬럼들이 있는지 있다면 당연히 제거를 해야한다. 그리고 scaling을 해서 표준편차 등의 수치들에 문제가 없는지를 파악하고 전반적으로 heatmap/pairplot으로 상관관계 여부를 확인해볼 필요가 있다는 것. 만약 회귀분석으로 예측 모델링을 했다면 regression OLS와 다중공선성 코드를 통해 컬럼간의 관계나 혹은 과적합 등의 문제가 발생하고 있는지 등을 파악하고 다시 전처리 과정을 거쳐야 한다. 이 부분들을 거치지 않으면 생각한대로 결과가 나오지 않는다는 것을 파이썬을 배우면서 깨달았지만 막상 실전이라 생각하고 연습하니 잘 안되었다… 연습이 얼마나 많이 필요한지도 알게 되었고 만약 실무에서 내가 이 데이터들을 다루고 모델링을 한다고 하면 엄청난 노력이 필요하지 않을까 싶다. Feature preprocessing과 EDA가 빅데이터 분석에서 엄청나게 중요하다는 것을 다시 한번 새삼 느껴본다.","categories":[],"tags":[]},{"title":"What I studied today (TIS)","slug":"What-I-studied-today-TIS","date":"2021-10-28T12:17:29.000Z","updated":"2021-10-28T12:21:39.242Z","comments":true,"path":"2021/10/28/What-I-studied-today-TIS/","link":"","permalink":"https://hesthers.github.io/archives/2021/10/28/What-I-studied-today-TIS/","excerpt":"","text":"TIS practice machine learning (Python codes) 빅분기 실기시험 대비 Topic: Apartment transaction price (Dacon data) Dacon data for practice I used the data here!","categories":[],"tags":[]},{"title":"basic statistics +","slug":"basic-statistics-1","date":"2021-10-27T04:35:23.000Z","updated":"2021-10-27T05:17:00.411Z","comments":true,"path":"2021/10/27/basic-statistics-1/","link":"","permalink":"https://hesthers.github.io/archives/2021/10/27/basic-statistics-1/","excerpt":"","text":"Basic statisticsThings to know the reason why the statistics is used (what I want to know from the statistical info) 데이터의 의미를 정확히 모를 때(when I need more knowledge about the field) 데이터 자체는 쉽게 파악이 되나 데이터 양이 많을 때 (if the volume of the info is vast, it is difficult to figure out the details) 측정을 잘못 했을 때 (noise is too many in info or need the reliability)를 대비해서 데이터에 대해 알기 위해 사용 the tendency of covariance the degree that two variances are distant from each mean value =&gt; as another one is moving forward the postive/negative direction being distant from its mean, one variance is also moving forward the positive direction being distant from its mean. When the deviation is multiplied, the output value is also larger. But, when the unit is different from each data, the scale of the deviation is also different. In other words, you cannot compare with these data, so you need to standardize the data first. correlation coefficient You cannot explain anything (e.g. the relation) only using the correlation coefficient. Correlation coefficient cannot describe the nonlinear relation. (You have to identify if there are direct correlation among the data first.) Because raw data originally includes the noise, you cannot figure out the perfect linear relation. In linear regressional coefficient, the errors are included.) the basic concepts of the sample compared to the population data should be unbiased. (if the specific biased information is identified from the sample, this information cannot explain about the population.) the features of the sample are same as of the population (you cannot predict at all if they are totally different.)","categories":[],"tags":[]},{"title":"basic statistics","slug":"basic-statistics","date":"2021-10-26T14:24:59.000Z","updated":"2021-10-26T14:40:29.843Z","comments":true,"path":"2021/10/26/basic-statistics/","link":"","permalink":"https://hesthers.github.io/archives/2021/10/26/basic-statistics/","excerpt":"","text":"Statisticsconcepts central tendency mean(average): dividing the sum of data by the count of the data median: 50% of a quantile or the data in the middle of the whole data mode: the frequent count of data quantile: 25%, 50%, 75% of the whole data measurement of distribution variance: how widely the whole data distribute std(standard deviation): the square root of variance coefficient correlation coefficient: the size of the relation between two variances / range must be within [-1, 1].the degree of the change to one variance as another one also changes covariance: the degree of how far both two variances are distant from each mean value / when one variance is in faraway point from its mean, how far another variance is distant from its mean regression Linear regression =&gt; regressional coefficient (the size that the independent variance impacts the dependent variance) important: minimizing the errors of data for the prediction","categories":[],"tags":[]},{"title":"when you need the breaktime","slug":"when-you-need-the-breaktime","date":"2021-10-25T05:44:43.000Z","updated":"2021-10-25T06:17:14.746Z","comments":true,"path":"2021/10/25/when-you-need-the-breaktime/","link":"","permalink":"https://hesthers.github.io/archives/2021/10/25/when-you-need-the-breaktime/","excerpt":"","text":"Sometimes, everyone needs some breaktime. When you have in trouble, you might want to give up everything, but I think everyday is beautiful! You deserve to be loved. Anne-Marie Beautiful 你敢有勇气，别放弃自己。你可以爱自己，能让自身够珍贵吧。","categories":[],"tags":[]},{"title":"algebra cosine distance","slug":"algebra-cosine-distance","date":"2021-10-24T05:53:48.000Z","updated":"2021-11-06T13:58:23.212Z","comments":true,"path":"2021/10/24/algebra-cosine-distance/","link":"","permalink":"https://hesthers.github.io/archives/2021/10/24/algebra-cosine-distance/","excerpt":"","text":"Cosine distance The relationship between cosine similarity and distance This is test image for this blog post.I wrote things on this file, so the original copyright to me.","categories":[],"tags":[{"name":"algebra","slug":"algebra","permalink":"https://hesthers.github.io/archives/tags/algebra/"}]},{"title":"data - vector algebra","slug":"data-vector-algebra","date":"2021-10-23T11:36:41.000Z","updated":"2021-10-23T11:47:19.112Z","comments":true,"path":"2021/10/23/data-vector-algebra/","link":"","permalink":"https://hesthers.github.io/archives/2021/10/23/data-vector-algebra/","excerpt":"","text":"data with algebraVector 3 terminology consisting of Vector basis means axis dimension means the amount of the feature (columns of a table) direction is related to unit vector (this literally means unit) direction is about physical trend. distance between vectors Euclidean distance: the sum of the absolute difference between two vectors Manhattan distance: the square root of the sum of the squared difference between two vectors Cosine distance: the multiplication of the two vectors divided by the sum of the scalar product of each vector cosine distance is related to cosine similarity.","categories":[],"tags":[{"name":"algebra","slug":"algebra","permalink":"https://hesthers.github.io/archives/tags/algebra/"}]},{"title":"Tableau pt.6 with project feedback","slug":"Tableau-pt-6-with-project-feedback","date":"2021-10-21T07:17:06.000Z","updated":"2021-10-21T07:25:54.623Z","comments":true,"path":"2021/10/21/Tableau-pt-6-with-project-feedback/","link":"","permalink":"https://hesthers.github.io/archives/2021/10/21/Tableau-pt-6-with-project-feedback/","excerpt":"","text":"Feedback of Tableau project 내가 가지고 싶어했던 이야기, 주제는 이런 것이였다… 이러한 부분들을 구현하기 위해서 이러한 내용들을 표현하고 싶은 것이였다. 보여지는 부분과 숫자의 매칭이 되지 않은 경우 혼동이 발생할 수 있음. 다른 그래프 끼리는 색상을 다르게 표현해주면 시각적으로 좋음. 인사이트 도출 (지도같은 시각화를 통해) window_max([~]) = [~] =&gt; 특정부분 색상 강조True/False로 구분되어 대시보드 탭 오른쪽으로 클릭하고 모든 시트 숨기기 기능 및 색상 구분 가능 눈에 띄는 변화를 색상으로 주는 것이 좋음. UI/UX 부분 고려사항임… 부동산에서는 평균보다는 중앙값에 focus.. 거래금액의 평균값.. 년도별은 있는 그대로 표현… top5 하이라이트 테이블 필터를 거래금액 평균이나 중앙값으로 설정 브랜드별 그룹화시켜서… 특정 이슈나 이벤트를 중점을 두고… Things to know about Tableau 데이터 셋 =&gt; 무엇에 대한 것인지 정확하게 알고 있어야 함. xxx.파일확장자 (카운트): 태블로에서 파일이 정확하게 올려졌는가태블로에서는 데이터를 집계해서 보여주기 때문에 디테일을 보기 어려우므로 하나하나 파악하기 어려움 데이터를 보여줄 때 보여주고 싶은 단면만 보여줘서는 안됨. Data Literacy 어떤 인사이트를 얻을것인가.. 데이터를 잘 읽어낼 수 있는가 데이터 시각화: graphicacy / 시각자료를 해석하는 능력 왜 중요한가?? 디지털로의 전환하는 시대 data literacy for data-driven decision making 데이터를 정확하게 이해하고 이를 기반으로 의사결정 할 수 있도록 해야 함. 보고싶어하는 방향, 가고싶어하는 방향을 지향하는지.. (확증편향 주의) 부분만 보고 전체인 것처럼 표현하는 것은 안됨. 본질을 제대로…. 데이터의 신뢰성 (신뢰할만한 자료인가.. reliability) = bias is in??","categories":[],"tags":[]},{"title":"Tableau pt.5","slug":"Tableau-pt-5","date":"2021-10-19T08:47:18.000Z","updated":"2021-10-19T08:48:08.061Z","comments":true,"path":"2021/10/19/Tableau-pt-5/","link":"","permalink":"https://hesthers.github.io/archives/2021/10/19/Tableau-pt-5/","excerpt":"","text":"Things to know about Tableau 태블로 프렙 =&gt; 데이터 전처리를 위해 필요한 프로그램 유니온 (union):combination of table as multiple table is combined and then create the whole table by adding the rows below.데이터 선택 창에서 드래그해서 해당 테이블 시트 부분을 끌다보면 유니온이라는 부분이 뜨게 됨.다른 컬럼이 생성된 경우 ctrl을 눌러서 해당 컬럼들을 동시에 클릭한다음 불일치 필드 병합시켜주고 이름은 수동으로 변경시켜줌. 조인 (Join):the combined data is added next to the new columnCombine the table physicallyBe careful when joining the data in each table are the same 100% (How to join the tables??) 방향 중요함. 조인의 조건 잘 맞춰주기 각 데이터 테이블의 공통적인 데이터를 가져올 것인가. 참고사항:관계: 느슨한 형태로 결합, 방향은 태블로가 알아서 잡아줌. (방향 자동 설정) 조건만 설정)블렌딩: 제약이 많으나 유용한 기능이며 따로 구분되어 있는 시트를 하나의 시트 위에 두 개의 원본소스를 같이 사용할 때 조건을 잡아주는 것, 연산을 통해서 개별 시각화를 만들 수 있으나 조인처럼 강력하게 물리적으로 결합되거나 병합이 일어나는 것은 아님. 지도상에서 읍면동인 경우 외부에서 지리경계 정보를 가져와야 함. (shp 파일 = 공간정보 파일) 엑셀 데이터 원본의 필드명을 인식못하는 경우 데이터 해석기 사용하면 됨. 자동으로 데이터 처리됨. change the data type because the join cannot occur when the type is dfferent from another one. 차원과 측정값의 경계가 모호하는 경우 태블로에서 사용하기 애매한 데이터 형이므로 피벗으로 테이블 형태를 변경하기 연도와 분기 순서가 뒤섞여있는 경우 날짜형으로 변환 (예. Q1 – 2018)RIGHT([Date], 4) + &quot;-&quot; + IF LEFT([Date],2) = &quot;Q1&quot; THEN &quot;01-01&quot; ELSEIF LEFT([Date],2) = &quot;Q2&quot; THEN &quot;04-01&quot; ELSEIF LEFT([Date],2) = &quot;Q3&quot; THEN &quot;07-01&quot; ELSE &quot;10-01&quot; END","categories":[],"tags":[{"name":"Tableau","slug":"Tableau","permalink":"https://hesthers.github.io/archives/tags/Tableau/"}]},{"title":"Tableau project record","slug":"Tableau-project-record","date":"2021-10-18T12:37:42.000Z","updated":"2021-10-18T12:42:31.108Z","comments":true,"path":"2021/10/18/Tableau-project-record/","link":"","permalink":"https://hesthers.github.io/archives/2021/10/18/Tableau-project-record/","excerpt":"","text":"Tableau project record It is just record for tableau project. Today record:collaborate members’ visualization and make sure of a total format for this project Maybe finish all things about project by tomorrow.","categories":[],"tags":[]},{"title":"Tableau project link","slug":"Tableau-project-link","date":"2021-10-17T05:33:15.000Z","updated":"2021-10-17T05:36:38.869Z","comments":true,"path":"2021/10/17/Tableau-project-link/","link":"","permalink":"https://hesthers.github.io/archives/2021/10/17/Tableau-project-link/","excerpt":"","text":"Project file LinkLink","categories":[],"tags":[]},{"title":"Tableau project","slug":"Tableau-project","date":"2021-10-16T12:29:22.000Z","updated":"2021-10-16T12:33:26.804Z","comments":true,"path":"2021/10/16/Tableau-project/","link":"","permalink":"https://hesthers.github.io/archives/2021/10/16/Tableau-project/","excerpt":"","text":"Tableau projectTableau project has begun! It is a small project again, a kind of toy project. Things of this projectTOPIC Aptarments of Seoul Real estate Scenario Recommendation to two groups (young newbie couple and 4050 gen. family) Factors for Analysis The year that the apt was built and how many people transact the apt, the avg.amounts based on city, county, the top 10 recommended apt, and the floor space Using Real Estate data of Seoul, the project would be proceeded!","categories":[],"tags":[{"name":"Tableau","slug":"Tableau","permalink":"https://hesthers.github.io/archives/tags/Tableau/"}]},{"title":"Tableau pt.4","slug":"Tableau-pt-4","date":"2021-10-14T10:02:07.000Z","updated":"2021-10-14T10:02:48.182Z","comments":true,"path":"2021/10/14/Tableau-pt-4/","link":"","permalink":"https://hesthers.github.io/archives/2021/10/14/Tableau-pt-4/","excerpt":"","text":"Tableau and prediction algorithm 태블로는 스토리텔링에는 굉장히 의미있고 효과적인 도구이나 예측 알고리즘 등에는 적절하지 못하나 예측은 추세 등의 간단한 기법을 활용해서 가능함 Things to know about Tableau 필터 (Filtering) 태블로의 본질과 맞닿아 있는 기능 (with 필터 액션) 스토리텔링이 가능하게 함 order of operations(태블로 개발자와 유저간의 약속)의 핵심 요소 (significant part of tableau) 차원필터(Dimensional filter): 가장 많이 사용하게 되는 필터, 테이블(차원) 값의 필터 단일값 = 하나의 값만 선택, 다중값 = 여러개의 선택지를 제공 측정값 필터: 통계표시 없으면 데이터 원본에서의 해당 데이터값은 아예 버리는 것 (한 행의 level에서 필터링), 통계표시 (집계 표시)가 있으면 제품의 측정값의 집계 방식으로 필터링 일반 탭: 원하는 값 선택, 와일드카드, 조건, 상위(정확한 기준을 제시, 이후에도 계속 조건이 유지됨.) 로직을 의미 independent filter: 서로 영향을 주지 않음. 차원 필터(분류)가 Top N(우선순위가 높은 필터)의 종속 필터이므로 ontext filter를 먼저 설정해줘야함. dimensional filter가 context filter로 올려줄 수 있는가.. (예. 지역을 컨텍스트로 설정) manually hidden filter: 숨기기 기능을 통해 전월대비 증감율 비율 표현이 가능함. 그러나 이후의 데이터를 업데이트하기 어려움. Table Calculation: LAST()라는 함수를 사용하면 테이블이 자동으로 필터링되어서 계산됨. 이중축 (Double axis) &amp; 결합축 (Combined axis) 한 행이나 열에 측정값이 두 개인 경우 (축이 두 개가 생성, 왼/오른쪽 혹은 그래프가 두 개가 생성) 원하는 데이터를 드래그해서 구현된 그래프 왼쪽이나 오른쪽으로 가져다 두면 두 개의 축이 양쪽에서 생성되어 자동으로 그래프를 표현 축의 스케일은 축 동기화로 기준 맞추어줄 수 있음 막대 크기를 조절해서 bar in bar 구현 가능하거나 축이 여러 개를 구현해줄 때 사용 마크카드가 따로따로 되어있는 이중축은 자유도가 높음 (개별 설정이 가능함.) 그러나 결합축인 경우 별도로 구현하기 어려움… 그러나 여러 가지 그래프를 한 번에 표현이 가능함. 비율이나 금액 등 단위가 다른 경우 결합축의 이중축으로 구현 혹은 별도로 각각 표현을 해야함. 도넛차트(Donut Chart) 파이차트에서 시작하며 두 개의 파이차트를 작업용, 유지용 두 개 생성 더미 필드에서 화면을 분할 생성해서 복사 = ctrl 키 누르면서 옆으로 드래그 작업용 필드에서 각도 부분 제거 및 세부정보등 변경 후 크기 조절하고 이중축으로 결합","categories":[],"tags":[]},{"title":"Tableau pt.3","slug":"Tableau-pt-3","date":"2021-10-13T11:40:44.000Z","updated":"2021-10-13T11:54:49.805Z","comments":true,"path":"2021/10/13/Tableau-pt-3/","link":"","permalink":"https://hesthers.github.io/archives/2021/10/13/Tableau-pt-3/","excerpt":"","text":"TABLEAUDashboard 제목을 텍스트 컨테이너로 작성하면 일부 공간을 차지하여 작성됨. 데이터를 집계해서 전체 정보를 요약해서 보여주므로 전체 현황을 보여주는 데 특화되어 있으나 구체적인 사항에 있어서는 약점이 있을 수 있음. 필터 액션: 깔때기 표시를 누르면 해당년도 등 기준에 따른 내용을 자세하게 보여줌. 필터 동작에서도 설정 가능함. Scatter plot &amp; Histogram &amp; Boxplot 산점도: 2개의 측정값의 관계 (correlation)를 나타내는 차트 측정값 두 개를 같이 올리게 되면 각 측정값의 한 개만 나오게 됨. (총 합계) 회귀선의 기울기: x축이 한 unit 증가할 때 y축의 증감량 측정값의 분포를 살펴볼 때 박스플롯 사용 해당 측정값의 구간차원을 생성하여 막대그래프 형성되며 불연속형 데이터로 자동 생성됨. 불연속형 데이터를 연속형으로 만들면 히스토그램으로 만들 수 있음 Map (Geographic graph) GIS: geographical info system Geospatial info on a map AcrGIS, QGIS 툴도 사용… BI (Business Intelligence): 태블로와 같은 도구들이 핵심적인 역할을 하고 있음. 지리 경계 정보는 어디에서 가져올 것인가… 만약 지역 명칭만 정확하게 가지고 있다면 태블로에서 빌려와서 사용이 가능함.단, 읍면동 단위와 같은 자세한 지역명칭의 경우 개인이 직접 가져와야 함. 텍스트 (혹은 문자열 데이터): 공간적 의미를 가지는 텍스트이나 지리적 역할을 부여함으로써 지도 상에 위치 등을 표시할 수 있음e.g. 강원도(Kang-won), 경기도(Kyung-ki), … 풀네임으로 되어있지 않은 데이터인 경우에도 지리적 역할을 부여함으로써 지도 인식이 가능함. 공식 지역명칭이 아닌 경우 (호남, 영남.. 등) 만들기 탭의 해당 원본 데이터를 기준으로 지리적 역할 부여해주어서 생성 가능함. County or City would be duplicated, so no mark on a map, unless it is uploaded with the states. If geographical data has both longtitude and latitude information, it could be represented on a map, but before you should change the string type of data into float (real number) type.","categories":[],"tags":[{"name":"Tableau","slug":"Tableau","permalink":"https://hesthers.github.io/archives/tags/Tableau/"}]},{"title":"Tableau pt.2","slug":"Tableau-pt-2","date":"2021-10-12T12:21:24.000Z","updated":"2021-10-12T12:41:39.251Z","comments":true,"path":"2021/10/12/Tableau-pt-2/","link":"","permalink":"https://hesthers.github.io/archives/2021/10/12/Tableau-pt-2/","excerpt":"","text":"TableauCharts of Tableau Bar chart (general) Pie chart (ratio, proportion) Line chart (time series data) Highlight table (highlight some data to emphasize) Things to know about these charts Trend is for a line chart &amp; Ratio is for a pie chart and including a bar chart, these charts are used in business a lot (Maybe.. 95%) in Tableau, the proper chart would be set automatically. Line Chart 날짜 타입에서 +/-표시는 date type을 의미 -&gt; 계층을 나타냄 (drill down/up) 년도별로 분절되어 있음 (long-term으로는 끊겨서 트렌드를 보여준다는 것은 즉, 불연속형으로 표현되어 있다는 것). 테이블 (혹은 차원)이 파란색으로 되어있으면 불연속형 데이터이고 초록색 표시인 경우 연속형 데이터를 의미. 그러나 연속형인 차원도 존재하고 불연속형인 측정값 또한 존재하고 있음. 월만 열에 나타내면 연도 상관없이 전체 년도의 데이터가 월 하나에 표시. 연속형 데이터는 연, 분기, 월 전부를 포함하고, 불연속형은 연, 분기, 월 따로따로 마우스 오른쪽을 누르면서 (윈도우환경) 드래그할 때 데이터 형 표시 선택가능함 라인차트와 영역차트의 차이는 그래프 아래에 색칠이 되어 있는가 아닌가 개별 차트의 전체, 분기별 트렌드를 한 번에 보여주고 싶을 때 영역차트가 적절함 추세선(Regression Line) represents long-term period of the data. Pie Chart how many the weight has in the very data예. 지역에서 매출이 얼마인가를 파악할 때 (전체 비율 중 얼마인가) 퀵 테이블 계산(QTC, Quick Table Calculation)을 가지고 비율 계산 가능함. The basic charts that Tableau has are built-in chart. 비율에서 모수(Parameter)가 중요함: 전체인지 어떤 기준에 따른 비율인지.. Dash Board 대시보드: 시트와 개체를 적절하게 배치할 수 있게 위치를 잡아주는 것.바둑판식: 전체 면적을 확보해가면서 배치 (알아서 사이즈 조정)개체 중 가/세로표시는 컨테이너 (담을 수 있는 공간)를 의미 index(): a function to set a line of data","categories":[],"tags":[{"name":"Tableau","slug":"Tableau","permalink":"https://hesthers.github.io/archives/tags/Tableau/"}]},{"title":"Tableau example","slug":"Tableau-example","date":"2021-10-11T01:29:53.000Z","updated":"2021-10-25T06:24:45.243Z","comments":true,"path":"2021/10/11/Tableau-example/","link":"","permalink":"https://hesthers.github.io/archives/2021/10/11/Tableau-example/","excerpt":"","text":"ExampleI made another example about Air BnB data using Tableau tool. This is just for test image on my Github blog by practicing Tableau.","categories":[],"tags":[{"name":"Tableau","slug":"Tableau","permalink":"https://hesthers.github.io/archives/tags/Tableau/"}]},{"title":"Today What I studied pt.4","slug":"Today-What-I-studied-pt-4","date":"2021-10-09T11:36:17.000Z","updated":"2021-10-09T11:49:37.988Z","comments":true,"path":"2021/10/09/Today-What-I-studied-pt-4/","link":"","permalink":"https://hesthers.github.io/archives/2021/10/09/Today-What-I-studied-pt-4/","excerpt":"","text":"TIS watched online lectures: Regression analysis (Multiple &amp; Simple) studied Chinese: Reading and Listening part-&gt; Watched Chinese News-&gt; Studied with book","categories":[],"tags":[]},{"title":"Tableau practice example","slug":"Tableau-practice-example-1","date":"2021-10-08T08:18:14.000Z","updated":"2021-10-08T08:21:49.558Z","comments":true,"path":"2021/10/08/Tableau-practice-example-1/","link":"","permalink":"https://hesthers.github.io/archives/2021/10/08/Tableau-practice-example-1/","excerpt":"","text":"Tableau exampleUsing Tableau tool, I made an example to show the data visualization with the downloaded data: USA COVID-19 death statistics and world indicator data. This is the link of my portfolio.: Tableau link","categories":[],"tags":[{"name":"Tableau","slug":"Tableau","permalink":"https://hesthers.github.io/archives/tags/Tableau/"}]},{"title":"Tableau pt.1","slug":"Tableau-pt-1","date":"2021-10-07T07:11:02.000Z","updated":"2021-10-07T07:32:23.668Z","comments":true,"path":"2021/10/07/Tableau-pt-1/","link":"","permalink":"https://hesthers.github.io/archives/2021/10/07/Tableau-pt-1/","excerpt":"","text":"TableauThe powerful, awesome tool for a data visualization TABLEAU!!!!!! How to design the information Through Tableau, it is possible to make/improve nice portfolio. easy to deliver the information and persuade the audience utilize in various fields (e.g. data journalism in the Press) possible to create value-added Things to know about Tableau csv 파일은 텍스트 파일에서 open drag and drop 방식 사용 (태블로에서 모든 동작 방식) 엑셀과 태블로 차이: 엑셀에서는 데이터 자체가 원본이므로 바로 수정가능하나 태블로는 원본 데이터 수정이 안됨 태블로 용어 필터: 데이터를 걸러서 보여줌. 대시보드: 정보 조합 상황판 (시트 크기 다르게 배치 가능), 하나의 화면에서 모니터링 가능함 스토리: ppt (차트 기반인 경우 장표(시트나 대시보드) 추가) 테이블 = 차원(기준, 관점) / nominal data 측정값 = numeric data 그룹핑: 정제되지 않은 데이터들을 구분 및 분류시 사용 서식 부분:영구적 방식: 숫자 형식 변경, 한번만 기본속성을 설정해주면 나머지 워크북의 모든 속성이 자동으로 설정됨.임시적 방식: 해당 열/행에서의 서식 =&gt; 패널에서 숫자형식 직접 변경 원본 데이터에서는 개별 데이터임에도 불구하고 태블로는 측정값을 항상 화면에서 반드시 집계, 합계되어서 보여줌. -&gt; 가볍게 요약해서 보여주는데 특화되어 있음 그래서 엑셀보다 우수 twbx 확장자로 저장 (저장이 안된 경우) - 프로가 아닌 경우 태블로 리더로 읽어야하는 파일 (바로 읽기 안됨!!) bar 차트 = 태블로 기본 차트 길이의 개념이 직관적으로 판단가능함 데이터를 태블로에서 시각화할 때: 차원과 측정값을 무엇으로 할 것인가 오른쪽 키를 누른 상태에서 시트로 드래그하면 통계방법을 묻는 작은 창이 나타남.","categories":[],"tags":[{"name":"Tableau","slug":"Tableau","permalink":"https://hesthers.github.io/archives/tags/Tableau/"}]},{"title":"Finishing Toy Project","slug":"Finishing-Toy-Project","date":"2021-10-06T09:05:04.000Z","updated":"2021-10-06T09:14:29.939Z","comments":true,"path":"2021/10/06/Finishing-Toy-Project/","link":"","permalink":"https://hesthers.github.io/archives/2021/10/06/Finishing-Toy-Project/","excerpt":"","text":"Finishing Toy ProjectThree Assumptions: 가설 1. 연령별 혹은 성별로 SNS 상에서 컨텐츠 언급에서 영향을 미치는 부분이 있다. 가설 2. SNS상의 해당 컨텐츠와 관련된 게시물 수와 네이버의 찜한 개수가 서로 상관 관계가 있을 것이다. 가설 3. SNS 검색량 혹은 게시물 수가 높을수록 OTT 오리지널 컨텐츠에 높은 영향력을 미칠 것이다. These assumptions are not validated. Results:Through SNS analysis, the proportion of the general distributing agency is much higher than others (Netflix and Tving original contents).","categories":[],"tags":[{"name":"Toy Project","slug":"Toy-Project","permalink":"https://hesthers.github.io/archives/tags/Toy-Project/"}]},{"title":"Toy Project last thing","slug":"Toy-Project-last-thing","date":"2021-10-05T14:46:21.000Z","updated":"2021-10-05T14:51:22.501Z","comments":true,"path":"2021/10/05/Toy-Project-last-thing/","link":"","permalink":"https://hesthers.github.io/archives/2021/10/05/Toy-Project-last-thing/","excerpt":"","text":"Toy Project record오늘이 마지막 토이프로젝트 준비기간이다.. 사실 3일 동안 토이프로젝트 크롤링 때문에 포스팅을 올리지 못했다. 토이프로젝트 준비 마지막 날로써 마지막 기록을 담아보려고 한다. Things that I did and to do 인스타그램 크롤링 완전히 마무리하고 결과 데이터는 csv에 저장하기 SNS 파트 시각화 끝내기 (다양한 시각화 기법 사용) 내일은 발표하는 날로 오전에 팀원들과 상의해서 그동안 준비해둔 코딩이며 시각화 부분 코딩 전부 합치기","categories":[],"tags":[{"name":"Toy Project","slug":"Toy-Project","permalink":"https://hesthers.github.io/archives/tags/Toy-Project/"}]},{"title":"Toy Project D3","slug":"Toy-Project-D3","date":"2021-10-01T13:03:33.000Z","updated":"2021-10-01T13:08:07.454Z","comments":true,"path":"2021/10/01/Toy-Project-D3/","link":"","permalink":"https://hesthers.github.io/archives/2021/10/01/Toy-Project-D3/","excerpt":"","text":"Day 3 Toy ProjectThis post is just for recording the process of toy project. 키노라이츠 크롤링 완료 후 데이터를 리스트에 저장하고 데이터 프레임으로 만든 다음 csv 파일로 저장하기 인스타 크롤링 시작","categories":[],"tags":[{"name":"Toy Project","slug":"Toy-Project","permalink":"https://hesthers.github.io/archives/tags/Toy-Project/"}]},{"title":"Toy Project 2","slug":"Toy-Project-2","date":"2021-09-30T09:50:13.000Z","updated":"2021-09-30T09:59:36.221Z","comments":true,"path":"2021/09/30/Toy-Project-2/","link":"","permalink":"https://hesthers.github.io/archives/2021/09/30/Toy-Project-2/","excerpt":"","text":"2nd Day of Toy Project 키노라이츠 사이트 크롤링: 날짜, 넷플릭스 및 티빙에서 방영된 컨텐츠 이름들 리스트로 저장 후 데이터 프레임 등으로 변환 이후 SNS에서 크롤링한 해당 컨텐츠 명들로 해시태그 수 등 크롤링 (화제성 파악 목적)","categories":[],"tags":[{"name":"Toy Project","slug":"Toy-Project","permalink":"https://hesthers.github.io/archives/tags/Toy-Project/"}]},{"title":"Toy Project 1","slug":"Toy-Project-1","date":"2021-09-29T13:00:45.000Z","updated":"2021-09-29T13:10:16.363Z","comments":true,"path":"2021/09/29/Toy-Project-1/","link":"","permalink":"https://hesthers.github.io/archives/2021/09/29/Toy-Project-1/","excerpt":"","text":"BEGIN toy projectRaw meeting report 요구사항기획 회의 리포트&lt;&lt;아이데이션&gt;&gt; 주제, 가설 (목표세우기) — 사이트, 수집 가능여부, 추출방안, 아이디어 및 어려움프로젝트 리포트 작성 내용:목표 — 한줄로 작성선정이유 (분석 동기 및 이유)데이터 수집 - 수집방법/출처 이슈사항 (데이터 전처리, 어려움 등…) 데이터 확인 — 결측값 등 설정데이터 분포 변수명 통일 토이 프로젝트 주제 및 방향 콘텐츠 화제성 비교 분석OTT 플랫폼의 오리지널 컨텐츠의 ott 시장에 미칠 영향력 분석 (디지털 미디어 컨텐츠의 화제성 파악) 관련 사이트:연관 사이트 크롤링으로 데이터 수집 (관련 정보 — 배급사, 개봉 혹은 방영 연도 등…)트렌드 사이트에서 중요 정보 파악 후 크롤링 등으로 수집sns에서 해시태그 등으로 화제성 요인 파악하기","categories":[],"tags":[{"name":"Toy Project","slug":"Toy-Project","permalink":"https://hesthers.github.io/archives/tags/Toy-Project/"}]},{"title":"Instagram Crawling","slug":"Instagram-Crawling","date":"2021-09-28T14:09:07.000Z","updated":"2021-09-28T14:24:21.331Z","comments":true,"path":"2021/09/28/Instagram-Crawling/","link":"","permalink":"https://hesthers.github.io/archives/2021/09/28/Instagram-Crawling/","excerpt":"","text":"Python Crawling Practice Example with Instagram with Selenium module in the python: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import timefrom bs4 import BeautifulSoupimport urllibdriver = webdriver.Chrome(&#x27;./chromedriver_win32/chromedriver.exe&#x27;)url = &#x27;https://www.instagram.com/&#x27;driver.get(url)time.sleep(4)ins_id = &#x27;[Enter your email for instagram id]&#x27; #or use &#x27;input&#x27; command here!pw = &#x27;[Enter your email for instagram pw]&#x27;driver.find_element_by_xpath(&#x27;[copy xpath here]&#x27;).send_keys(ins_id)driver.find_element_by_xpath(&#x27;[copy xpath here]&#x27;.send_keys(pw)driver.find_element_by_xpath(&#x27;[copy xpath here]&#x27;).click()time.sleep(3)driver.find_element_by_xpath(&#x27;[copy xpath here]&#x27;).click()time.sleep(3)driver.find_element_by_xpath(&#x27;[copy xpath here]&#x27;).click()time.sleep(4)key_word = input(&#x27;키워드를 입력하세요 : &#x27;)driver.find_element_by_xpath(&#x27;[copy xpath here]&#x27;).send_keys(key_word)time.sleep(4)driver.find_element_by_xpath(&#x27;[copy xpath here]&#x27;).click()for i in range(50): if i == 0: driver.find_element_by_xpath(&#x27;[copy xpath here]&#x27;).click() html = BeautifulSoup(driver.page_source, &#x27;html.parser&#x27;) html.select(&#x27;div &gt; div.C4VMK &gt; span&#x27;) tag = [tag.text.strip(&#x27;#&#x27;) for tag in html.select(&#x27;a.xil3i&#x27;)] comment = [comment.text for comment in html.select(&#x27;div &gt; div.C4VMK &gt; span&#x27;)] like = [l.text for l in html.select(&#x27;section.EDfFK.ygqzn &gt; div &gt; div &gt; a &gt; span&#x27;)] image = driver.find_element_by_css_selector(&#x27;[copy selector here related to the image]&#x27;) image_url = image.get_attribute(&#x27;src&#x27;) urllib.request.urlretrieve(image_url, &#x27;./test_image.jpg&#x27;) time.sleep(5) print(&#x27;크롤링 시작&#x27;) driver.find_element_by_xpath(&#x27;[copy xpath here]&#x27;).click() else: html = BeautifulSoup(driver.page_source, &#x27;html.parser&#x27;) html.select(&#x27;div &gt; div.C4VMK &gt; span&#x27;) total_tag = [tag.text.strip(&#x27;#&#x27;) for tag in html.select(&#x27;a.xil3i&#x27;)] extra_comment = [comment.text for comment in html.select(&#x27;div &gt; div.C4VMK &gt; span&#x27;)] total_like = [l.text for l in html.select(&#x27;section.EDfFK.ygqzn &gt; div &gt; div &gt; a &gt; span&#x27;)] try: image = driver.find_element_by_css_selector(&#x27;[copy selector here related to the image]&#x27;) image_url = image.get_attribute(&#x27;src&#x27;) urllib.request.urlretrieve(image_url, f&#x27;./test_image&#123;i&#125;.jpg&#x27;) except Exception as error: image = driver.find_element_by_css_selector(&#x27;[copy selector here related to the image]&#x27;) image_url = image.get_attribute(&#x27;src&#x27;) urllib.request.urlretrieve(image_url, f&#x27;./test_image&#123;i&#125;.jpg&#x27;) driver.find_element_by_xpath(&#x27;[copy xpath here]&#x27;).click() time.sleep(10) print(f&#x27;크롤링 중 &#123;1+i&#125;&#x27;) No unauthorized usage and copy (This is against!)","categories":[],"tags":[{"name":"Python Crawling","slug":"Python-Crawling","permalink":"https://hesthers.github.io/archives/tags/Python-Crawling/"}]},{"title":"Crawling without BeautifulSoup","slug":"Crawling-without-BeautifulSoup","date":"2021-09-27T11:49:30.000Z","updated":"2021-09-27T12:13:32.907Z","comments":true,"path":"2021/09/27/Crawling-without-BeautifulSoup/","link":"","permalink":"https://hesthers.github.io/archives/2021/09/27/Crawling-without-BeautifulSoup/","excerpt":"","text":"Crawling pt.3Today github blog’s topic is about crawling without BeautifulSoup module. Generally, when using crawling technique, I use BeautifulSoup module. Unfortunately, most websites has the informational security problems of the crawling, so the crawling is blocked. To avoid the block of the website, use crawling technique of the dynamic web page. Use F12 key on your keyboard and find the referer webpage address and user agent on the network panel. Write python codes of the crawling the following: 1234567891011url = &#x27;[web address to crawl]&#x27;info = &#123; &#x27;referer&#x27;: &#x27;[main webpage address]&#x27;, &#x27;user-agent&#x27;: &#x27;[user agent on the network panel of the developer webpage]&#x27;&#125;response = requests.get(url, headers=info)# response.textimport jsondata = json.loads(response.text)data You can access the dynamic webpage that is blocked by java script(js).","categories":[],"tags":[]},{"title":"Things to do tomorrow","slug":"The-things-to-do-tomorrow","date":"2021-09-25T14:49:30.000Z","updated":"2021-09-25T14:55:10.233Z","comments":true,"path":"2021/09/25/The-things-to-do-tomorrow/","link":"","permalink":"https://hesthers.github.io/archives/2021/09/25/The-things-to-do-tomorrow/","excerpt":"","text":"Tomorrow Plans This is just for planning note of mine. Things to do tomorrow Solve the mock test on a book (certificate) — a series of 3~4 mock exams Study the machine learning on online lecture and write new worksheet. Review Ch.1 and solve the practice questions Study much harder and keep going! D-day is left 6 days… 加油！！","categories":[],"tags":[]},{"title":"Today What I Studied","slug":"TIS/Today-What-I-Studied","date":"2021-09-24T12:01:58.000Z","updated":"2021-09-24T12:06:43.339Z","comments":true,"path":"2021/09/24/TIS/Today-What-I-Studied/","link":"","permalink":"https://hesthers.github.io/archives/2021/09/24/TIS/Today-What-I-Studied/","excerpt":"","text":"Today What I Studied (TIL) Big Data Analysis Engineer Certificate: Solved the practice questions and Reviewed what I studied before (pt.2 about statistics part) I bought new practice questions book yesterday, so I started solving the questions today!","categories":[],"tags":[]},{"title":"Just for note","slug":"Just-for-note","date":"2021-09-23T10:02:23.000Z","updated":"2021-09-23T10:12:27.357Z","comments":true,"path":"2021/09/23/Just-for-note/","link":"","permalink":"https://hesthers.github.io/archives/2021/09/23/Just-for-note/","excerpt":"","text":"This post is just for self note. 빅분기 시험이 곧 다가왔다. 1주일하고 2일?? 총 9일 정도 남았다. 그래서 과목별 중요하다고 느끼는 주제들만 살짝 적어볼까 싶다. 사실은 적을 주제가 딱히 없어서다.. 너무 영어로만 포스팅을 적다보니 한글로도 적어보고 싶었다. 1과목: 데이터 아키텍처(특히 HDFS 부분들), SW 수집기술, 개인정보 및 비식별화, 사회적인 IT관련 이슈들, 데이터 거버넌스 2과목: 기술, 추론 통계 파트, 파생변수, 확률 계산, EDA, 데이터 분류 3과목: 분석기법들 (인공신경망, 회귀분석, SVM, 비정형데이터, 시계열, 앙상블 기법, 랜덤포레스트, 군집분석) 4과목: 평가지표(confusion matrix), 교차 검증, 적합도 검정, 모델 선정 및 평가, 시각화 (종류 및 특징들), 분석 결과 활용, 비즈니스 기여도 평가 (평가기법 부분), 분석모형 모니터링 및 리모델링 구분","categories":[],"tags":[]},{"title":"Interesting News Article","slug":"Interesting-News-Article","date":"2021-09-22T08:22:00.000Z","updated":"2021-09-22T08:29:01.076Z","comments":true,"path":"2021/09/22/Interesting-News-Article/","link":"","permalink":"https://hesthers.github.io/archives/2021/09/22/Interesting-News-Article/","excerpt":"","text":"Interesting Article MetaverseI found the interesting news article about Metaverse. This is the link: News Link Metaverse is one of topics that I have had an interest in. In financial field, especially digital finance, most companies have been interested in this topic. If metaverse is boosted in various field, the business would have the new type of market.","categories":[],"tags":[]},{"title":"Today What I Studied","slug":"TIS/Today-What-I-Studied-2","date":"2021-09-21T13:02:05.000Z","updated":"2021-09-21T13:10:00.453Z","comments":true,"path":"2021/09/21/TIS/Today-What-I-Studied-2/","link":"","permalink":"https://hesthers.github.io/archives/2021/09/21/TIS/Today-What-I-Studied-2/","excerpt":"","text":"Today What I StudiedI studied the national authorized certificate (Big Data Engineer Certificate) Solved Pt.3 questions (about 50 questions) studied pt.4 Evaluating analysis of the modeling confusion matrix ROC curve standards of the evaluation of analyzing the modeling cross validation - Holdout, K-Fold, LOOCV parametric significance test goodness of fit test I also watched online lecture videos about machine learning(linear regression) Topic: AirBnB","categories":[],"tags":[]},{"title":"Today What I Studied","slug":"TIS/Today-What-I-Studied-1","date":"2021-09-20T12:00:28.000Z","updated":"2021-09-20T12:15:25.707Z","comments":true,"path":"2021/09/20/TIS/Today-What-I-Studied-1/","link":"","permalink":"https://hesthers.github.io/archives/2021/09/20/TIS/Today-What-I-Studied-1/","excerpt":"","text":"Today What I Studied (TIS) I studied the national authorized certificate (Big Data Engineer Certificate) pt.3 Bigdata modeling — analysis technique (From unstructured data to non-parametric statistics) I also watched online lecture videos about data visualization with seaborn module of python and web crawling part. I practiced time series data analysis in python. (posted it on my Naver blog)","categories":[],"tags":[]},{"title":"Python Crawling Practice.2","slug":"python_review/Python-Crawling-Practice-2","date":"2021-09-18T12:25:21.000Z","updated":"2021-09-18T12:31:30.064Z","comments":true,"path":"2021/09/18/python_review/Python-Crawling-Practice-2/","link":"","permalink":"https://hesthers.github.io/archives/2021/09/18/python_review/Python-Crawling-Practice-2/","excerpt":"","text":"Crawling Practice pt.2오늘은 아주아주 재미있는 주제로 도전해보려고 한다… 바로바로 !!!!! 크롤링이다!! 이번에는 주제 단어를 입력해서 포털 사이트를 연결해서 관련 주제에 대한 정보들을 불러오는 걸 해볼 생각이다. (크롤링 수업 때 배운 내용과 코드를 활용해 반복문으로 바꿔서 추가로 해봤다.) 주제는 드라마다… 드라마 제목을 입력받아 관련 정보를 출력하고 리스트 등에 저장해서 이걸 분석해볼 생각이다.. (사실 정보관련 해서 문제 발생을 방지하기 위해 이 포스팅을 복제하거나 하는 그런 건 방지할 필요가 있을 것 같다… 윤리적인 부분 포함해서 이 포스팅으로 개인적인 이익을 얻거나 하는 일은 전혀 없다!!!!) 크롤링하기 전에 당연히 BeautifulSoup 패키지가 있어야하고 크롤링 준비를 위한 코드들을 확인해야한다.. 이번에 해볼 분석은 우선 드라마 이름을 입력하면 해당 드라마의 줄거리와 배역 정보를 출력하고 이에 대한 관련된 키워드들이 포함된 네이버 카페, 블로그의 링크를 뽑아보려고 한다. 단, 드라마는 10개만 저장해서 뽑을 생각이다. 이 정보들을 전부 csv파일에 저장할 생각이다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import requests from bs4 import BeautifulSoup import timeimport pandas as pdcnt = 0drama_nm = []drama_txt = []drama_info = []drama_link = []while True: cnt += 1 drama = input(&#x27;드라마 이름을 입력하세요 : &#x27;) drama_nm.append(drama) url = f&#x27;https://search.daum.net/search?w=tot&amp;DA=YZR&amp;t__nil_searchbox=btn&amp;sug=&amp;sugo=&amp;sq=&amp;o=&amp;q=&#123;drama&#125;&#x27; response = requests.get(url) html = BeautifulSoup(response.text, &#x27;html.parser&#x27;) time.sleep(2) drama_summary = html.select(&#x27;dd.cont&#x27;)[0].text drama_info.append(drama_summary) print(f&#x27;드라마 줄거리 : &#123;drama_summary&#125;&#x27;) print() print(&#x27; 배역 배우&#x27;) print(&#x27; ------------------&#x27;) for cont in html.select(&#x27;div#tv_casting li&#x27;)[1:]: drama_info.append(cont.text) print(cont.text) print() url1 = f&#x27;https://search.naver.com/search.naver?where=nexearch&amp;sm=top_sug.pre&amp;fbm=1&amp;acr=1&amp;acq=%EA%B2%80%EC%9D%80&amp;qdt=0&amp;ie=utf8&amp;query=&#123;drama&#125;&#x27; res = requests.get(url1) html1 = BeautifulSoup(res.text, &#x27;html.parser&#x27;) html1.select(&#x27;a&#x27;) for txt in html1.select(&#x27;a&#x27;): if drama in txt.text: if (&#x27;cafe&#x27; in txt.attrs[&#x27;href&#x27;]) or (&#x27;blog&#x27; in txt.attrs[&#x27;href&#x27;]): drama_txt.append(txt.text) drama_link.append(txt.attrs[&#x27;href&#x27;]) print(f&quot;&#123;txt.text&#125;: 링크 =&gt; &#123;txt.attrs[&#x27;href&#x27;]&#125;&quot;) else: pass else: pass time.sleep(5) dr_l = &#123;&#x27;drama_nm&#x27;: drama_nm, &#x27;drama_txt&#x27;: drama_txt, &#x27;drama_info&#x27;: drama_info, &#x27;drama_link&#x27;: drama_link&#125; for l, k in zip(dr_l.values(), dr_l.keys()): dr = pd.DataFrame(l) dr.to_csv(f&#x27;&#123;k&#125;.csv&#x27;, index=False, encoding = &#x27;utf8&#x27;) if cnt &gt; 10: breakprint()time.sleep(2)print(drama_nm)print()print(drama_info)print()print(drama_txt)print()print(drama_link) Against unauthorized copy and deployment.","categories":[],"tags":[]},{"title":"Crawling Practice (feat.Python)","slug":"python_review/Crawling-Practice-feat-Python","date":"2021-09-17T06:14:38.000Z","updated":"2021-09-17T06:49:01.917Z","comments":true,"path":"2021/09/17/python_review/Crawling-Practice-feat-Python/","link":"","permalink":"https://hesthers.github.io/archives/2021/09/17/python_review/Crawling-Practice-feat-Python/","excerpt":"","text":"Crawling Practice with the topic Drama123456789101112131415161718192021222324252627kw = input(&#x27;키워드를 입력하세요 : &#x27;)url = f&#x27;https://search.naver.com/search.naver?where=nexearch&amp;sm=top_sug.pre&amp;fbm=1&amp;acr=1&amp;acq=%EA%B2%80%EC%9D%80&amp;qdt=0&amp;ie=utf8&amp;query=&#123;kw&#125;&#x27;response = requests.get(url)html = BeautifulSoup(response.text, &#x27;html.parser&#x27;)html.select(&#x27;a&#x27;)for txt in html.select(&#x27;a&#x27;): if kw in txt.text: if (&#x27;cafe&#x27; in txt.attrs[&#x27;href&#x27;]) or (&#x27;blog&#x27; in txt.attrs[&#x27;href&#x27;]): print(txt.text, txt.attrs[&#x27;href&#x27;]) else: pass else: passdrama = input(&#x27;드라마 이름을 입력하세요 : &#x27;)url = f&#x27;https://search.daum.net/search?w=tot&amp;DA=YZR&amp;t__nil_searchbox=btn&amp;sug=&amp;sugo=&amp;sq=&amp;o=&amp;q=&#123;drama&#125;&#x27;response = requests.get(url)html = BeautifulSoup(response.text, &#x27;html.parser&#x27;)drama_summary = html.select(&#x27;dd.cont&#x27;)[0].textprint(f&#x27;드라마 줄거리 : &#123;drama_summary&#125;&#x27;)print()print(&#x27; 배역 배우&#x27;)print(&#x27; ------------------&#x27;)for cont in html.select(&#x27;div#tv_casting li&#x27;)[1:]: print(cont.text) I used the crawling python code to print drama info. 무단복제 금지(Against unauthorized copies)","categories":[],"tags":[]},{"title":"Data Visualization","slug":"python_review/Data-Visualization","date":"2021-09-16T08:02:40.000Z","updated":"2021-09-16T08:24:17.775Z","comments":true,"path":"2021/09/16/python_review/Data-Visualization/","link":"","permalink":"https://hesthers.github.io/archives/2021/09/16/python_review/Data-Visualization/","excerpt":"","text":"VISUALIZATIONPython has provided various visualization tools. You can draw various graphs in python.Before you draw graphs or plots, you have to import modules for visualization.12import matplotlib.pyplot as pltimport seaborn as sns Difference between matplotlib and seabornMatplotlib is not much aesthetic as much as seaborn module. If you want to make your graphs more beautiful, I recommend you would use seaborn module.Moreover, you also use much more various graphs that matplotlib module does not provide. TYPES of graphs Line Plot Bar Plot Histogram Scatter Plot Box Plot Extra graphs that seaborn module provides countplot distplot jointplot pairplot AND … kdeplot etc,… more graphs!!! FIND HERE! Matplotlib website &amp; Plotly website &amp; Seaborn website Using graphsIt is important to use the proper graphs to analyze the data better depending on the proper purpose. For example, if you want to express the density (which point are show more or less on graph), you should use the histogram. Or when you want to express the purchase amount by language that the international consumers use on graph, you should use the barplot or countplot of seaborn module.","categories":[],"tags":[]},{"title":"Pandas (pt.2)","slug":"python_review/Pandas-pt-2","date":"2021-09-14T07:57:15.000Z","updated":"2021-09-14T08:56:24.158Z","comments":true,"path":"2021/09/14/python_review/Pandas-pt-2/","link":"","permalink":"https://hesthers.github.io/archives/2021/09/14/python_review/Pandas-pt-2/","excerpt":"","text":"Fancy Indexing (Masking) of Python Through bool type of array, indexing the multidimensional array When extracting some data samples that are needed for the data analysis You will use this method a lot for data analysis Combining two tables use the code merge or concat and create the pivot table based on the key, merge tables (SQL: Join) inner/outer join left_on/right_on: the different key names left/right concat: just combine two tables based on axis (axis = 0: concat based on the rows / axis = 1: concat based on the columns) Editing Indexes/Columns Editing Indexes: use the code .reset_index(drop = True(or False), inplace = True(or False)) Editing Columns: use the code .drop(&#39;[column name]&#39;, axis = 1) or del (data_name)[&#39;column name&#39;] Changing Column name: .rename(columns = &#123;&#39;orginal column name&#39;:&#39;column name to change&#39;&#125;, inplace = True(or False)) If you do not want to use the variable name, use the inplace = True! Data Sampling and Analysis When using the basic indexing, slicing, the conditional sampling(masking/fancy indexing), the data analysis will be nice!! You can also extract the meaningful information in those data. Pandas with the high-quality analysis usage of the apply function with the lambda function make the function with def and apply it to columns and apply function broad casting and data masking","categories":[],"tags":[]},{"title":"Python Pandas","slug":"python_review/Python-Pandas","date":"2021-09-13T14:13:41.000Z","updated":"2021-09-13T14:25:58.808Z","comments":true,"path":"2021/09/13/python_review/Python-Pandas/","link":"","permalink":"https://hesthers.github.io/archives/2021/09/13/python_review/Python-Pandas/","excerpt":"","text":"Python PandasWhat it is … A kind of python packages to deal with data as table format This package is good for data analysis. Most for data scientists Importing this package command import pandas as pd DataFrame one of data type in python table format provides various statistical, visualization functions used to read and save data as files — support various file format (e.g. csv, xlsx …)123pd.read_csv(&#x27;./[file_name]&#x27;, encoding = &#x27;utf8&#x27;)pd.to_csv(&#x27;./[file_name]&#x27;, encoding = &#x27;utf8&#x27;, index = False) If you are using Mac Book, you do not have to write ‘utf8’ to encode files. Indexing, Slicing data This is the same as of List type, Numpy. iloc and loc is added in data frame type. You can also find out the data type of vector-data is the series type when the code for columns or rows is written. You can also find the data based on the columns. (or one column) Fancy Indexing can be applied to here!! To be continued…","categories":[],"tags":[]},{"title":"Numpy practice with python","slug":"python_review/Numpy-practice-with-python","date":"2021-09-11T09:10:33.000Z","updated":"2021-09-11T09:16:06.543Z","comments":true,"path":"2021/09/11/python_review/Numpy-practice-with-python/","link":"","permalink":"https://hesthers.github.io/archives/2021/09/11/python_review/Numpy-practice-with-python/","excerpt":"","text":"Practice questionsQ:Make Fibonacci sequence python code with Numpy module. I already posted the answer code (my solution!) on my Naver blog.Please refer to this blog after solving this problem. You can also utilize defining function in python. Solution: Blog Link","categories":[],"tags":[]},{"title":"Python Numpy","slug":"python_review/Python-Numpy","date":"2021-09-10T08:34:17.000Z","updated":"2021-09-11T05:06:30.854Z","comments":true,"path":"2021/09/10/python_review/Python-Numpy/","link":"","permalink":"https://hesthers.github.io/archives/2021/09/10/python_review/Python-Numpy/","excerpt":"","text":"Numpy in PythonWhat is Numpy? For numeric calculation of linear algebra and scientific computing opensource so anyone can install and use Numpy package(module) for free broadcasting: the data that has the different data type(shape) is possible to calculate with other data (vector) Why is Numpy used? Numpy is one of the powerful tools in python for calculation Functions in Numpy module are very useful! Some functions of Numpy arithmetic calculation function prod() or cumprod(): mutliply sum() or cumsum(): add abs(): absolute value square() or sqrt() exp() (= exponential) or log() statistic calculation function mean() / std() / var() / max() or min() logical calculation function isnan()/unique() geometric calculation function shape() transpose() Array in Numpy the basic data type of numpy calculation The calculation is relatively fast. Indexing and Slicing in array of Numpy This is the same as in the list data type. Special shape of array np.ones() np.zeros() the indentical matrixThe two things, ones() and zeros() would be frequently used in data analysis.","categories":[],"tags":[]},{"title":"Python practice questions","slug":"python_review/Python-practice-questions","date":"2021-09-09T13:16:46.000Z","updated":"2021-09-09T15:06:41.094Z","comments":true,"path":"2021/09/09/python_review/Python-practice-questions/","link":"","permalink":"https://hesthers.github.io/archives/2021/09/09/python_review/Python-practice-questions/","excerpt":"","text":"Practice QuestionsLottery example: Pick(Print) 6 random numbers without the duplication within the range between 1 and 45. python code: 12345678910111213141516import randomlotto_list = []index = []lotto = []for i in range(6): index.append(i) lotto_list.append(random.randint(1, 45))for n, m in zip(index, lotto_list): lotto.append(m) cnt = lotto.count(lotto[n]) if cnt &gt;= 2: lotto.remove(lotto[n]) lotto.append(random.randint(1, 45)) else: lotto = lottolotto Similar question: Make 5 sets for a lottery. 123456789101112131415161718192021total_lotto = []for i in range(5): lotto_list = [] index = [] lotto = [] for i in range(6): index.append(i) lotto_list.append(random.randint(1, 45)) for n, m in zip(index, lotto_list): lotto.append(m) cnt = lotto.count(lotto[n]) if cnt &gt;= 2: lotto.remove(lotto[n]) lotto.append(random.randint(1, 45)) else: lotto = lotto if len(lotto) == 6: break total_lotto.append(lotto)total_lotto","categories":[],"tags":[]},{"title":"Python pt.2","slug":"python_review/Python-conditional-statement-and-iteration-statement","date":"2021-09-08T12:43:32.000Z","updated":"2021-09-08T13:32:57.020Z","comments":true,"path":"2021/09/08/python_review/Python-conditional-statement-and-iteration-statement/","link":"","permalink":"https://hesthers.github.io/archives/2021/09/08/python_review/Python-conditional-statement-and-iteration-statement/","excerpt":"","text":"conditional statement and iteration statementPython conditional statement Literally, the conditional statement can be used in the conditions of data analysis on Python. The reserved word of conditional statement is if/elif/else. if: The first condition is true. elif: if not true in the first condition, but other conditions are true. else: when every above conditions are false Format with example: 123456if a &gt; 15: print(&#x27;High&#x27;)elif a = 15: print(&#x27;Yes!&#x27;)else: print(&#x27;Low&#x27;) Conditional statement is useful in various codes. (You’ll use it a lot!!) Python iteration statement while: repeat infinitely not until the reserved word break is used (in other words, repeat until conditions are met) format:123while [conditions (able to be without conditions)]: statement.. ~ if you stop the while statement because the conditions are alreay met, you have to use the reserved word break. if you execute the statement and move on the next statement by passing only some specific condition, you can use the reserved word continue. for: repeat during the defined number of times use the range function with for loop (the automatically defined number of times) format:123for i in [list, tuple ...] (or range(#, #+1): statement.. ~","categories":[],"tags":[]},{"title":"Data analysis for Python","slug":"python_review/Data-analysis-for-Python","date":"2021-09-07T13:27:04.000Z","updated":"2021-09-07T13:42:08.048Z","comments":true,"path":"2021/09/07/python_review/Data-analysis-for-Python/","link":"","permalink":"https://hesthers.github.io/archives/2021/09/07/python_review/Data-analysis-for-Python/","excerpt":"","text":"Data analysis with PythonWhy python for Data analysis? what values the analyst extracts what the analyst want to get from data analysis whether the analyst can make the right decision The process of data analysis data collection (using open source/crawling …) data exploration (EDA methods with python) data preprocessing (using raw data or feature engineering) data modeling and feedback The tools for data analysis The most important thing: use the proper tool according to the purpose of data analysis Need to know why you analyze the data and which functions are required Python A kind of programming (command to a computer with some specific language) language as interpreter language (executing line by line) For communication between a human and computer readability: easy to recognize and read codes open source for python code: libraries and modules (e.g. numpy, pandas …) automatically management of a memory in a computer variables in python codes = space to save some data","categories":[],"tags":[]},{"title":"python pt.1","slug":"python_review/python-pt-1","date":"2021-09-06T12:28:55.000Z","updated":"2021-09-06T12:57:08.997Z","comments":true,"path":"2021/09/06/python_review/python-pt-1/","link":"","permalink":"https://hesthers.github.io/archives/2021/09/06/python_review/python-pt-1/","excerpt":"","text":"Pythonreference python link Definition of Python It is used for various purpose: data analysis, program engineering interpreter language (compatible computing language) easier to learn than other languages Types of variables int(integer) float bool(True/False) str(string-characters) Operators The normal operation (calculation) is possible to not only numbers but also strings. example:12print(&#x27;Hello!&#x27; *3)# The result: Hello!Hello!Hello! Logical operators and comparative operators are also possible in python. Data structures: 4 types List format: [] (separation with ‘,’(comma)) indexing with slicing (This is the most important thing!!! in data analysis): python indexing .append(): add the data in the list .sort() or .sort(reverse=True): sorting the data by the ascending or descending order .reverse: printing data by the reverse order of the list data Dictionary format: {keys:values} values in dictionary type: list type, set type, strings, int/float print the values using the indexing update or .append(): add or change the data Tuple format: () (separation with ‘,’(comma)) The data is fixed, so changing the data is impossible. Adding the data is possible. Set format: {} (similar to dictonary but no keys and values) remove the duplication (the unique data): if you add the same data a lot, the only one data is added into the data set. the operation: intersection, union, difference","categories":[],"tags":[]},{"title":"SQL MongoDB","slug":"SQL_review/SQL-MongoDB","date":"2021-09-05T06:23:59.000Z","updated":"2021-09-05T06:51:48.667Z","comments":true,"path":"2021/09/05/SQL_review/SQL-MongoDB/","link":"","permalink":"https://hesthers.github.io/archives/2021/09/05/SQL_review/SQL-MongoDB/","excerpt":"","text":"MongoDB What is MongoDB?It is a kind of NoSQL, no relation between tables.Java Script is for MongoDB’s query. Terminology on NoSQL tables = collections documents = records create databases = use [database name] query in MySQL = begin with Select but query in NoSQL = db.[collection name].find() if there are some conditions about data, add the conditions in (). operator begins with $. comparative operator: $lt = &lt; (less than), $lte = &lt;= (less than and equal) $gt = &gt; (greater than), $gte = &gt;= (greater than and equal) $eq = = (is equal) logical operator: $or, $and, $not, $nor function: begin withvar [variance name] = function(~){ formula return ~} making an order = sort: 1 = asc, -1 = desc, .sort({column name: 1 or -1}) skip(#).limit(#) = limit #, # in MySQL please check the extra examples and the formula in depth on my Naver blog and Github blog!","categories":[],"tags":[]},{"title":"list of my blog","slug":"list-of-my-blog","date":"2021-09-04T08:36:48.000Z","updated":"2021-09-04T08:39:20.486Z","comments":true,"path":"2021/09/04/list-of-my-blog/","link":"","permalink":"https://hesthers.github.io/archives/2021/09/04/list-of-my-blog/","excerpt":"","text":"List of my Github blogI appended the link of my github blog list on this page! Please check here if you cannot find all posts. My Github Blog link","categories":[],"tags":[]},{"title":"SQL replication","slug":"SQL_review/SQL-replication","date":"2021-09-03T14:49:18.000Z","updated":"2021-09-03T15:16:39.330Z","comments":true,"path":"2021/09/03/SQL_review/SQL-replication/","link":"","permalink":"https://hesthers.github.io/archives/2021/09/03/SQL_review/SQL-replication/","excerpt":"","text":"SQL REPLICATIONI learned about SQL replication today, so I’m going to post this topic today on my blog. definition of Replication:Imagine about copy. The replication on SQL is the same as copy. the process of Replication: First, you have to create each master and slave server on AWS website. You can also create various slave servers. Second, using the public IP addresses of master and slave server, connect each server on git bash.Use the command ssh -i ~/.ssh/[key pair name].pem ubuntu@[public IP address]. (You should install ubuntu before.) Third, on master and slave server, install MySQL and connect MySQl and server.Create the new account of MySQL on git bash. Fourth, on MySQL that connected with master server on git bash, create database. Fifth, come back MySQL program and create new connections with master and slave servers. Sixth, on the master DB connection of MySQL, type some queries to connect with the slave DB connection. Seventh, insert some data on the master DB. Eighth, check if the replication is successful on the slave DB connection. If you see the yes on the slave status, you succeed!!!!! You’re done!","categories":[],"tags":[]},{"title":"SQL BACKUP","slug":"SQL_review/SQL-BACKUP","date":"2021-09-02T08:17:07.000Z","updated":"2021-09-02T08:45:23.224Z","comments":true,"path":"2021/09/02/SQL_review/SQL-BACKUP/","link":"","permalink":"https://hesthers.github.io/archives/2021/09/02/SQL_review/SQL-BACKUP/","excerpt":"","text":"SQL Backup You can back the files or databases up on MySQL using the server. If you delete some data by mistake, you can restore them, but only under the condition that you have to back them up before. types: depending on the environment of the backup hot backup: when executing DB on SQL cold backup: when stopping the execution of DB on SQL depending on the way to back up logical backup: store data by changing into SQL Query, easy to check the errors, use when storing the data is a few amount physical backup: take the relatively large size of file, the fast speed of the backup and the restoration, use when storing large amount of data and backing them up as soon as possible hot logical backup: using crontab and shell script, check the logs and times for the backup with cyberduck, upload the files or databases to back up cold physical backup: when you have the authorized access before you backing the data up you have to log out the ubuntu account then access to again you have to make the AWS server for the backup first IF you feel that using the virtual server, it is difficult to back the data up, you can back the data up on SQL program. (Export data)","categories":[],"tags":[]},{"title":"SQL join","slug":"SQL_review/SQL-join","date":"2021-09-01T14:32:35.000Z","updated":"2021-09-01T14:50:28.996Z","comments":true,"path":"2021/09/01/SQL_review/SQL-join/","link":"","permalink":"https://hesthers.github.io/archives/2021/09/01/SQL_review/SQL-join/","excerpt":"","text":"SQL Join Join means that a table is connected with another table or other tables through a specific, mutual columns. types: inner join left join outer join: left/right join &amp; union right join if you make a join on tables, write “(inner)/left/right join ~ on” in a query. union: remove the duplicated data if you make a join on three tables, you can write the names of three tables next to the reserved word “from”.","categories":[],"tags":[]},{"title":"SQL constraints","slug":"SQL_review/SQL-constraints","date":"2021-08-31T13:44:49.000Z","updated":"2021-08-31T13:58:24.480Z","comments":true,"path":"2021/08/31/SQL_review/SQL-constraints/","link":"","permalink":"https://hesthers.github.io/archives/2021/08/31/SQL_review/SQL-constraints/","excerpt":"","text":"SQL constraints when making a table on SQL, you can make constraints on each columns. unique: you cannot save the same data on a table. It is the only data such as SSN, not duplicated. primary: this key has the attributes of Not Null and Unique. foreign: when connecting a table with another table, you can constrain another table from adding other data. if you break the foreign key of constraints, it also breaks the integrity. Extra constraints not null: you cannot save the table without data. default: the values that you always have, it is the primary value (the primary key and primary value is different here!) when having constraints, you can apply tables to use and analyze data better.","categories":[],"tags":[]},{"title":"SQL basics","slug":"SQL_review/SQL-basics","date":"2021-08-30T11:21:53.000Z","updated":"2021-08-30T11:36:20.806Z","comments":true,"path":"2021/08/30/SQL_review/SQL-basics/","link":"","permalink":"https://hesthers.github.io/archives/2021/08/30/SQL_review/SQL-basics/","excerpt":"","text":"SQL Basics when using SQL, begin with select or SELECT with the end ;(semi-colon). example: 123select name, nofrom infowhere no &gt;= 13; basic sentence to execute SQL (This sentence is the same as in all RDBMS.) 12345SELECT columns...FROM table names[WHERE condition statement #using the condition statement here! (with arithmetic operator)ORDER BY columns... #asc (= default) or descLIMIT ...]; between: in the range if the number is more than 15, less than 24,write like the following: 123select ~~from ~~where no between 15 and 24; in/not in: in = including, not in = not including like: when the specific string is included in data","categories":[],"tags":[]},{"title":"python function","slug":"python_review/python-function","date":"2021-08-28T06:17:21.000Z","updated":"2021-08-28T06:36:08.395Z","comments":true,"path":"2021/08/28/python_review/python-function/","link":"","permalink":"https://hesthers.github.io/archives/2021/08/28/python_review/python-function/","excerpt":"","text":"Practice for function Question: Enter your name and print name with the greeting using function. 12345678def greeting(n): if n.isalpha(): return &quot;Dear &quot; + name + &quot;, Hello!&quot; elif n.isdigit() or n.isalnum(): print(&#x27;Is it your name?&#x27;) name = str(input(&quot;Please input your name: &quot;))greeting(name)","categories":[],"tags":[]},{"title":"python example code","slug":"python_review/python-example-code","date":"2021-08-27T12:44:03.000Z","updated":"2021-08-27T13:06:59.660Z","comments":true,"path":"2021/08/27/python_review/python-example-code/","link":"","permalink":"https://hesthers.github.io/archives/2021/08/27/python_review/python-example-code/","excerpt":"","text":"Python example codePractice question for Python 문제: 연필 한 다스는 연필 12개이다. 원하는 만큼의 수를 입력받아 그 수는 연필 몇 다스이고 몇 개인지 출력하기 1234pencil = int(input(&#x27;Enter the number of pencils: &#x27;))print(&#x27;입력한 연필 수는 %d다스&#x27; % (pencil // 12), end = &#x27;,&#x27;)print(&#x27; 나머지는 %d개입니다.&#x27; % (pencil % 12))","categories":[],"tags":[]},{"title":"My other blog","slug":"My-other-blog","date":"2021-08-26T11:52:59.000Z","updated":"2021-08-26T12:13:21.335Z","comments":true,"path":"2021/08/26/My-other-blog/","link":"","permalink":"https://hesthers.github.io/archives/2021/08/26/My-other-blog/","excerpt":"","text":"My naver blog linkNaver Blog link It is my another blog on Naver website. I posted things for some python projects and basics. If you are also interested in Chinese, please visit my Naver blog! You’ll find some nice posts, too!","categories":[],"tags":[]},{"title":"Practice posting","slug":"Practice-posting","date":"2021-08-25T13:18:09.000Z","updated":"2021-08-25T13:53:24.201Z","comments":true,"path":"2021/08/25/Practice-posting/","link":"","permalink":"https://hesthers.github.io/archives/2021/08/25/Practice-posting/","excerpt":"","text":"The following list is the list of my favorite songs: My favorite songs: ‘What to do’ by Dean feat.Crush, Jeff BernatYouTube MV ‘Know Me Too Well’ by New Hope Club with Danna PaolaYouTube MV ‘At My Worst’ by Pink Sweat$ feat.KehlaniYouTube MV ‘On The Ground’ by ROSÉ (BlackPink)YouTube MV ‘Daylight’ by KozyPop (Song by kenessi, floryy)YouTube MV ‘I Do Too’ by The ReklawsYouTube lyrics video ‘Celebrity’ by IUYouTube MV ‘Blueming’ by IUYouTube MV ‘pov’ by Ariana GrandeYouTube lyrics video ‘Don’t Let Her’ by Walker HayesYouTube MV ‘Life Goes On’ by BTSYouTube MV I recommend this songs! I really like these songs. I do not remember all songs, so I could not put more songs on the list…","categories":[],"tags":[]},{"title":"My first post","slug":"My-first-post","date":"2021-08-24T01:04:04.000Z","updated":"2021-08-24T01:11:44.237Z","comments":true,"path":"2021/08/24/My-first-post/","link":"","permalink":"https://hesthers.github.io/archives/2021/08/24/My-first-post/","excerpt":"","text":"This is my first hexo blog post. favorite animals: doggies cats Go to google","categories":[],"tags":[]}],"categories":[],"tags":[{"name":"TIL","slug":"TIL","permalink":"https://hesthers.github.io/archives/tags/TIL/"},{"name":"project","slug":"project","permalink":"https://hesthers.github.io/archives/tags/project/"},{"name":"journal review","slug":"journal-review","permalink":"https://hesthers.github.io/archives/tags/journal-review/"},{"name":"Bigdata certificate","slug":"Bigdata-certificate","permalink":"https://hesthers.github.io/archives/tags/Bigdata-certificate/"},{"name":"practice for machine learning","slug":"practice-for-machine-learning","permalink":"https://hesthers.github.io/archives/tags/practice-for-machine-learning/"},{"name":"algebra","slug":"algebra","permalink":"https://hesthers.github.io/archives/tags/algebra/"},{"name":"Tableau","slug":"Tableau","permalink":"https://hesthers.github.io/archives/tags/Tableau/"},{"name":"Toy Project","slug":"Toy-Project","permalink":"https://hesthers.github.io/archives/tags/Toy-Project/"},{"name":"Python Crawling","slug":"Python-Crawling","permalink":"https://hesthers.github.io/archives/tags/Python-Crawling/"}]}